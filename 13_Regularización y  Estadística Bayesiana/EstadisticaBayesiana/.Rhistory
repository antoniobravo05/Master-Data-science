```{r load regression libraries, message=FALSE}
library(glmnet)
```
### `glmnet` package
This package performs linear regression with regularization. The documentation can be found in the [R package documentation](https://cran.r-project.org/web/packages/glmnet/index.html). It has been developed by professors at Stanford University, and authors of "The Elements of Statistical Learning" book, one of the basic of Machine Learning.
It does not only fit Linear Regression, but logisctic, multinomial, poisson and Cox regression as well.
Main function is `glmnet`, that solves the following problem:
$$
\min_{\beta_0,\beta} \frac{1}{N} \sum_{i=1}^{N} w_i \, l(y_i,\, \beta_0+\beta^T \textbf{x}_i) + \lambda\left[(1-\alpha) \, ||\beta||_2^2/2 + \alpha \, ||\beta||_1\right],
$$
```
glmnet(x, y, family=c("gaussian","binomial","poisson","multinomial","cox","mgaussian"),
weights, offset=NULL, alpha = 1, nlambda = 100,
lambda.min.ratio = ifelse(nobs<nvars,0.01,0.0001), lambda=NULL,
standardize = TRUE, intercept=TRUE, thresh = 1e-07,  dfmax = nvars + 1,
pmax = min(dfmax * 2+20, nvars), exclude, penalty.factor = rep(1, nvars),
lower.limits=-Inf, upper.limits=Inf, maxit=100000,
type.gaussian=ifelse(nvars<500,"covariance","naive"),
type.logistic=c("Newton","modified.Newton"),
standardize.response=FALSE, type.multinomial=c("ungrouped","grouped"))
```
* It automatically searches for several `lambda`s, the regularization parameter.
* `alpha` plays an important role, because it's the mechanism to choose between Ridge (`alpha=0`) and Lasso (`alpha=1`).
* Linear regression corresponds to `family='gaussian'` (the default) ($w_i=1/2$). It has to do with the relation between Regularized Linear Regression and Bayesian Gaussian Estimation (not covered in the course).
* `standardize=TRUE` by default.
* NOTE: If alpha=1, it is had a LASSO, if alpha=0 it's had a RIDGE.
## First problem
We will analyze the effect of regularization on generated data, where we can control how the data is correlated.
Data is $y=X\beta + \epsilon$, where $\epsilon$ represents a Gaussian noise $\epsilon ~ N(0,I)$. The difference in the models is how the $\beta$ is.
* $N$=number of data points
* $P$=dimension of Beta (dimension of our data).
* $p$=number of true predictors: $\beta = (1, \dots_p, 1, 0, \dots_{P-p}, 0)^T$. Our model only will have p parameters. The P parameters become relevant due to the noise added, before that these were zeros.
### 1. Small signal. Lots of noise
We now have many $\beta$ which are 0, so we will only have noise.
* p=15
* n=1000
```{r generate data 1}
n <- 1000  # Number of observations
p <- 5000  # Number of predictors included in model
real_p <- 15  # Number of true predictors
#x <- data.frame(matrix(rnorm(n*p), nrow=n, ncol=p)
x <- matrix(rnorm(n*p), nrow=n, ncol=p)
#y <- rowSums(x[,1:real_p]) + rnorm(n)
y <- apply(x[,1:real_p], 1, sum) + rnorm(n)
# Split data into train (2/3) and test (1/3) sets
train_rows <- sample(1:n, .66*n)
x.train <- x[train_rows, ]
x.test <- x[-train_rows, ]
y.train <- y[train_rows]
y.test <- y[-train_rows]
```
```{r metrics}
RMSE <- function(response, predicted){
return (sqrt(mean((response-predicted)^2, na.rm = TRUE)))
}
MSE <- function(response, predicted){
return (mean((response-predicted)^2, na.rm = TRUE))
}
```
```{r linear regression}
df_ <- data.frame(x.train, y=y.train)
df_test_ <- data.frame(x.test, y=y.test)
fit.linear <- lm(y ~ .,  data = df_) #
#predict.linear <- predict(fit.linear, newdata = data.frame(x.test))
predict.linear <- predict(fit.linear, newdata =df_test_)
rmse = RMSE(predict.linear, y.test)
mse = MSE(predict.linear, y.test)
hist(fit.linear$residuals, breaks = 30) # normally distributed :)
fit.lasso <- glmnet(x.train, y.train, family="gaussian", alpha=1)
fit.ridge <- glmnet(x.train, y.train, family="gaussian", alpha=0)
par(mfrow=c(1,2))
plot(fit.lasso, xvar="lambda")
plot(fit.ridge, xvar="lambda")
n <- 1000  # Number of observations
p <- 5000  # Number of predictors included in model
real_p <- 15  # Number of true predictors
x <- matrix(rnorm(n*p), nrow=n, ncol=p)
#y <- rowSums(x[,1:real_p]) + rnorm(n)
y <- apply(x[,1:real_p], 1, sum) + rnorm(n)
# Split data into train (2/3) and test (1/3) sets
train_rows <- sample(1:n, .66*n)
x.train <- x[train_rows, ]
x.test <- x[-train_rows, ]
y.train <- y[train_rows]
y.test <- y[-train_rows]
RMSE <- function(response, predicted){
return (sqrt(mean((response-predicted)^2, na.rm = TRUE)))
}
MSE <- function(response, predicted){
return (mean((response-predicted)^2, na.rm = TRUE))
}
df_ <- data.frame(x.train, y=y.train)
df_test_ <- data.frame(x.test, y=y.test)
fit.linear <- lm(y ~ .,  data = df_) #
fit.linear <- lm(y ~ .,  data = df_) #
#predict.linear <- predict(fit.linear, newdata = data.frame(x.test))
predict.linear <- predict(fit.linear, newdata =df_test_)
rmse = RMSE(predict.linear, y.test)
mse = MSE(predict.linear, y.test)
hist(fit.linear$residuals, breaks = 30) # normally distributed :)
fit.lasso <- glmnet(x.train, y.train, family="gaussian", alpha=1)
fit.lasso <- glmnet(x.train, y.train, family="gaussian", alpha=1)
fit.ridge <- glmnet(x.train, y.train, family="gaussian", alpha=0)
par(mfrow=c(1,2))
plot(fit.lasso, xvar="lambda")
plot(fit.ridge, xvar="lambda")
par(mfrow=c(1,2))
plot(fit.lasso, xvar="lambda")
plot(fit.ridge, xvar="lambda")
fit.lasso
y.pred.lasso <- predict(fit.lasso, newx = x.test, type = "response", s = 0.1)
y.pred.lasso <- predict(fit.lasso, newx = x.test, type = "response", s = 0.1)
y.pred.lasso <- predict(fit.lasso, newx = x.test, type = "response", s = 0.1)
y.pred.ridge <- predict(fit.ridge, newx = x.test, type = "response", s = 0.1)
# s is lambda. Its initialization.
rmse.lasso <- RMSE(y.pred.lasso, y.test)
mse.lasso <- MSE(y.pred.lasso, y.test)
rmse.ridge <- RMSE(y.pred.ridge, y.test)
mse.ridge <- MSE(y.pred.ridge, y.test)
print (paste0('Linear Regression. RMSE=', round(rmse,2), ', MSE=', round(mse,2)))
print (paste0('Lasso. RMSE=', round(rmse.lasso,2), ', MSE=', round(mse.lasso,2)))
print (paste0('Ridge RMSE=', round(rmse.ridge,2), ', MSE=', round(mse.ridge,2)))
sum(coef(fit.ridge, s=0.1)>0)
sum(coef(fit.lasso, s=0.1)>0)
lasso.cv <- cv.glmnet(x.train, y.train, alpha = 1) # CV to obtain best lambda
lasso.cv <- cv.glmnet(x.train, y.train, alpha = 1) # CV to obtain best lambda
ridge.cv <- cv.glmnet(x.train, y.train, alpha = 0) # CV to obtain best lambda
par(mfrow=c(1,2))
plot(lasso.cv, main="LASSO")
plot(ridge.cv, main="Ridge")
y.pred.lasso <- predict(lasso.cv, s = lasso.cv$lambda.1se, newx = x.test, type = "response")
y.pred.ridge <- predict(ridge.cv, s = ridge.cv$lambda.1se, newx = x.test, type = "response")
rmse.lasso.opt <- RMSE(y.pred.lasso, y.test)
mse.lasso.opt <- MSE(y.pred.lasso, y.test)
rmse.ridge.opt <- RMSE(y.pred.ridge, y.test)
mse.ridge.opt <- MSE(y.pred.ridge, y.test)
print (paste0('Linear Regression. RMSE=', round(rmse,2), ', MSE=', round(mse,2)))
print (paste0('Lasso. RMSE=', round(rmse.lasso,2), ', MSE=', round(mse.lasso,2)))
print (paste0('Lasso OPT. RMSE=', round(rmse.lasso.opt,2), ', MSE=', round(mse.lasso.opt,2)))
print (paste0('Ridge RMSE=', round(rmse.ridge.opt,2), ', MSE=', round(mse.ridge.opt,2)))
print (paste0('Ridge OPT RMSE=', round(rmse.ridge.opt,2), ', MSE=', round(mse.ridge.opt,2)))
n <- 1000  # Number of observations
p <- 5000  # Number of predictors included in model
real_p <- 1500  # Number of true predictors
x <- matrix(rnorm(n*p), nrow=n, ncol=p)
# Split data into train (2/3) and test (1/3) sets
train_rows <- sample(1:n, .66*n)
x.train <- x[train_rows, ]
x.test <- x[-train_rows, ]
y.train <- y[train_rows]
y.test <- y[-train_rows]
RMSE <- function(response, predicted){
return (sqrt(mean((response-predicted)^2, na.rm = TRUE)))
}
MSE <- function(response, predicted){
return (mean((response-predicted)^2, na.rm = TRUE))
}
df_ <- data.frame(x.train, y=y.train)
df_test_ <- data.frame(x.test, y=y.test)
fit.linear <- lm(y ~ .,  data = df_) #
fit.linear <- lm(y ~ .,  data = df_) #
#predict.linear <- predict(fit.linear, newdata = data.frame(x.test))
predict.linear <- predict(fit.linear, newdata =df_test_)
rmse = RMSE(predict.linear, y.test)
mse = MSE(predict.linear, y.test)
hist(fit.linear$residuals, breaks = 30) # normally distributed :)
fit.lasso <- glmnet(x.train, y.train, family="gaussian", alpha=1)
fit.lasso <- glmnet(x.train, y.train, family="gaussian", alpha=1)
fit.ridge <- glmnet(x.train, y.train, family="gaussian", alpha=0)
par(mfrow=c(1,2))
plot(fit.lasso, xvar="lambda")
par(mfrow=c(1,2))
plot(fit.lasso, xvar="lambda")
plot(fit.ridge, xvar="lambda")
par(mfrow=c(1,2))
plot(fit.lasso, xvar="lambda")
plot(fit.ridge, xvar="lambda")
y.pred.lasso <- predict(fit.lasso, newx = x.test, type = "response", s = 0.1)
y.pred.ridge <- predict(fit.ridge, newx = x.test, type = "response", s = 0.1)
# s is lambda. Its initialization.
rmse.lasso <- RMSE(y.pred.lasso, y.test)
mse.lasso <- MSE(y.pred.lasso, y.test)
rmse.ridge <- RMSE(y.pred.ridge, y.test)
mse.ridge <- MSE(y.pred.ridge, y.test)
print (paste0('Linear Regression. RMSE=', round(rmse,2), ', MSE=', round(mse,2)))
print (paste0('Lasso. RMSE=', round(rmse.lasso,2), ', MSE=', round(mse.lasso,2)))
print (paste0('Ridge RMSE=', round(rmse.ridge,2), ', MSE=', round(mse.ridge,2)))
sum(coef(fit.ridge, s=0.1)>0)
sum(coef(fit.lasso, s=0.1)>0)
lasso.cv <- cv.glmnet(x.train, y.train, alpha = 1) # CV to obtain best lambda
lasso.cv <- cv.glmnet(x.train, y.train, alpha = 1) # CV to obtain best lambda
ridge.cv <- cv.glmnet(x.train, y.train, alpha = 0) # CV to obtain best lambda
lasso.cv <- cv.glmnet(x.train, y.train, alpha = 1) # CV to obtain best lambda
ridge.cv <- cv.glmnet(x.train, y.train, alpha = 0) # CV to obtain best lambda
par(mfrow=c(1,2))
plot(lasso.cv, main="LASSO")
plot(ridge.cv, main="Ridge")
library(e1071) #naive bayes
# misc
library(ggplot2)
library(dplyr)
library(caret)
library(foreign)
dd <- read.dta('data/hsbdemo.dta')
head(dd)
ggplot(dd, aes(x=prog)) + geom_bar()
library(corrplot)
install.packages("corrplot")
library(corrplot)
dd.int <- dd %>% select(read, write, math, science, socst)
corrplot(cor(dd.int))
mydd <- dd
trainIndex <- createDataPartition(mydd$prog, p=0.7, list = FALSE)
train <- mydd[trainIndex, ]
test <- mydd[-trainIndex, ]
nb.model <- naiveBayes(prog ~ science + socst, data=train)
print(nb.model)
pred.train <- predict(nb.model, newdata=train, type="class")
train.table <- table(train$prog, pred.train)
pred.test <- predict(nb.model, newdata=test, type="class")
test.table <- table(test$prog, pred.test)
acc.train <- (train.table[1,1]+train.table[2,2]+train.table[3,3])/sum(train.table)
acc.test <- (test.table[1,1]+test.table[2,2]+test.table[3,3])/sum(test.table)
print("Contingency Table for Training Data")
print(train.table)
print("Contingency Table for Test Data")
print(test.table)
nb.model2 <- naiveBayes(prog ~ science + socst + ses + female + schtyp,
data=train)
pred.train <- predict(nb.model2, newdata=train, type="class")
train.table <- table(train$prog, pred.train)
pred.test <- predict(nb.model2, newdata=test, type="class")
test.table <- table(test$prog, pred.test)
acc.train <- (train.table[1,1]+train.table[2,2]+train.table[3,3])/sum(train.table)
acc.test <- (test.table[1,1]+test.table[2,2]+test.table[3,3])/sum(test.table)
print(paste0("Accuracy. Train: ", round(acc.train,3), ", test: ", round(acc.test,3)))
if(!require("dummies")){
install.packages("dummies")
library("dummies")
}
setwd("~/Documents/DATA SCIENCE MASTER VM/Apredizaje no supervisado")
movies = read.table("data/movies.txt",header=TRUE, sep="|",quote="\"")
str(movies)
View(movies)
View(movies)
View(movies)
head(movies)
tail(movies)
table(movies$Comedy)
#cuantas hay de comedia, de western y romance
table(movies$Comedy)
table(movies$Western)
table(movies$Romance, movies$Drama)
##### 4. Bloque de calculo de distancias #####
#calculamos las distancias de todas las categorias
distances = dist(movies[2:20], method = "euclidean")
clusterMovies = hclust(distances, method = "ward.D")
dev.off()
plot(clusterMovies)
rect.hclust(clusterMovies, k=2, border="yellow")
rect.hclust(clusterMovies, k=3, border="blue")
rect.hclust(clusterMovies, k=4, border="green")
rect.hclust(clusterMovies, k=6, border="green")
rect.hclust(clusterMovies, k=6, border="orange")
#Ya se cuantos grupos tendré, ahora que nombre les doy, como los interpreto?
#
NumCluster=10
#Ya se cuantos grupos tendré, ahora que nombre les doy, como los interpreto?
#
NumCluster=6
#Ya se cuantos grupos tendré, ahora que nombre les doy, como los interpreto?
#Definimos el nº de clusters:
NumCluster=2
rect.hclust(clusterMovies, k=NumCluster, border="red")
movies$clusterGroups = cutree(clusterMovies, k = NumCluster)
table(movies$clusterGroups)
#vemos de cada grupo que tipo de peliculas hay
tapply(movies$Action, movies$clusterGroups, mean)
tapply(movies$Adventure, movies$clusterGroups, mean)
tapply(movies$Animation, movies$clusterGroups, mean)
tapply(movies$Childrens, movies$clusterGroups, mean)
tapply(movies$Comedy, movies$clusterGroups, mean)
tapply(movies$Crime, movies$clusterGroups, mean)
tapply(movies$Documentary, movies$clusterGroups, mean)
tapply(movies$Drama, movies$clusterGroups, mean)
#¿Como es el grupo 1 y el grupo 2?
aggregate(.~clusterGroups,FUN=mean, data=movies)
#Ya se cuantos grupos tendré, ahora que nombre les doy, como los interpreto?
#Definimos el nº de clusters:
NumCluster=4
rect.hclust(clusterMovies, k=NumCluster, border="red")
movies$clusterGroups = cutree(clusterMovies, k = NumCluster)
table(movies$clusterGroups)
#vemos de cada grupo que tipo de peliculas hay. Si queremos ver una peli de accion, tengo que ir al grupo 1, pero no todas son de aacion.
tapply(movies$Action, movies$clusterGroups, mean)
table(movies$clusterGroups)
#vemos de cada grupo que tipo de peliculas hay. Si queremos ver una peli de accion, tengo que ir al grupo 1, pero no todas son de aacion.
tapply(movies$Action, movies$clusterGroups, mean)
tapply(movies$Adventure, movies$clusterGroups, mean)
tapply(movies$Animation, movies$clusterGroups, mean)
tapply(movies$Childrens, movies$clusterGroups, mean)
tapply(movies$Comedy, movies$clusterGroups, mean)
tapply(movies$Crime, movies$clusterGroups, mean)
tapply(movies$Documentary, movies$clusterGroups, mean)
tapply(movies$Drama, movies$clusterGroups, mean)
#¿Como es el grupo 1 y el grupo 2?
#Con Ncluster=2, el segundo grupo es interesante, son todo drama. En el algoritmo jerarquico no es necesario poner/fijar semilla.
aggregate(.~clusterGroups,FUN=mean, data=movies)
#Ya se cuantos grupos tendré, ahora que nombre les doy, como los interpreto?
#Definimos el nº de clusters:
NumCluster=6
rect.hclust(clusterMovies, k=NumCluster, border="red")
movies$clusterGroups = cutree(clusterMovies, k = NumCluster)
table(movies$clusterGroups)
aggregate(.~clusterGroups,FUN=mean, data=movies)
subset(movies, Title=="Men in Black (1997)")
cluster2 = subset(movies, movies$clusterGroups==2)
cluster2$Title[1:10]
View(cluster2)
View(cluster2)
cluster7 = subset(movies, movies$clusterGroups==7)
cluster2$Title[1:10]
creditos=read.csv("data/creditos.csv",stringsAsFactors = FALSE)
str(creditos)
head(creditos)
summary(creditos)
##### 10. Bloque de tratamiento de variables #####
#cambiamos las variables caracter a dummys
creditosNumericos=dummy.data.frame(creditos, dummy.class="character" )
#esta funcion hace el zscore (normaliza)
creditosScaled=scale(creditosNumericos)
View(creditosScaled)
View(creditosScaled)
#no sabemos los clusters a hacer, tenemos que probar, k=1, k=2...
NUM_CLUSTERS=2
set.seed(1234)
Modelo=kmeans(creditosScaled,NUM_CLUSTERS)
creditos$Segmentos=Modelo$cluster
creditosNumericos$Segmentos=Modelo$cluster
table(creditosNumericos$Segmentos)
aggregate(creditosNumericos, by = list(creditosNumericos$Segmentos), mean)
#no sabemos los clusters a hacer, tenemos que probar, k=1, k=2...
NUM_CLUSTERS=3
set.seed(1234)
Modelo=kmeans(creditosScaled,NUM_CLUSTERS)
creditos$Segmentos=Modelo$cluster
creditosNumericos$Segmentos=Modelo$cluster
table(creditosNumericos$Segmentos)
aggregate(creditosNumericos, by = list(creditosNumericos$Segmentos), mean)
#no sabemos los clusters a hacer, tenemos que probar, k=1, k=2...
NUM_CLUSTERS=4
set.seed(1234)
Modelo=kmeans(creditosScaled,NUM_CLUSTERS)
creditos$Segmentos=Modelo$cluster
creditosNumericos$Segmentos=Modelo$cluster
table(creditosNumericos$Segmentos)
aggregate(creditosNumericos, by = list(creditosNumericos$Segmentos), mean)
#no sabemos los clusters a hacer, tenemos que probar, k=1, k=2...
NUM_CLUSTERS=5
set.seed(1234)
Modelo=kmeans(creditosScaled,NUM_CLUSTERS)
creditos$Segmentos=Modelo$cluster
creditosNumericos$Segmentos=Modelo$cluster
table(creditosNumericos$Segmentos)
aggregate(creditosNumericos, by = list(creditosNumericos$Segmentos), mean)
##### 13. Bloque de Metodo de seleccion de numero de clusters (Elbow Method) #####
#esta es la regla del codo. Se suman los errores (distancia respecto al centroide de todos los puntos)
Intra <- (nrow(creditosNumericos)-1)*sum(apply(creditosNumericos,2,var))
for (i in 2:15) Intra[i] <- sum(kmeans(creditosNumericos, centers=i)$withinss)
plot(1:15, Intra, type="b", xlab="Numero de Clusters", ylab="Suma de Errores intragrupo")
coches=mtcars # Base de datos ejemplo en R
str(coches)
head(coches)
summary(coches)
modelo_bruto=lm(mpg~.,data=coches)
summary(modelo_bruto)
cor(coches)
modelo1=lm(mpg~cyl,data=coches)
summary(modelo1)
modelo2=lm(mpg~disp,data=coches)
summary(modelo2)
modelo3=lm(mpg~hp,data=coches)
summary(modelo3)
modelo4=lm(mpg~drat,data=coches)
summary(modelo4)
modelo5=lm(mpg~wt,data=coches)
summary(modelo5)
modelo6=lm(mpg~qsec,data=coches)
summary(modelo6)
modelo7=lm(mpg~vs,data=coches)
summary(modelo7)
modelo8=lm(mpg~am,data=coches)
summary(modelo8)
modelo9=lm(mpg~gear,data=coches)
summary(modelo9)
modelo10=lm(mpg~carb,data=coches)
summary(modelo10)
cor(coches)
modelo11=lm(mpg~               ,data=coches)
modelo11=lm(mpg~cyl+wt,data=coches)
summary(modelo11)
summary(modelo_bruto)
modelo=step(modelo_bruto,direction="both",trace=1)
#teóricamente el mejor modelo es Step:  AIC=61.31
#mpg ~ wt + qsec + am
summary(modelo)
PCA<-prcomp(coches[,-c(1)],scale. = TRUE)
summary(PCA)
#me indica los valores propios. Con PC2 tendríamos el 84% de la informacion.
#Con PC5 te quedas con la mitad de variables y tenemos el 95% de informacion de la varianza
#pero hay que ver el tradeoff, no hay nada que sea mejor o peor, todo dependerá de lo que queramos asumir.
#al final es un criterio, pero no hay un criterio.
plot(PCA)
cor(coches)
cor(PCA$x)
biplot(PCA)
PCA$rotation
coches$PCA1=PCA$x[,1]
coches$PCA2=PCA$x[,2]
coches$PCA3=PCA$x[,3]
modelo_PCA=lm(mpg~PCA1,data=coches)
summary(modelo_PCA)
modelo_PCA=lm(mpg~PCA$x,data=coches)
summary(modelo_PCA)
#modelo con la 1 y con la 3
modelo_PCA=lm(mpg~PCA1+PCA3,data=coches)
summary(modelo_PCA)
#cuando las variables son incorreladas, no tiene efecto en los coeficientes estimados el cambiar las variables.
#con esta tecnica se ha transfromado el modelo con las mismas caracteristicas usando solo 2 variables.
#ahora, es dificil interpretar estas variables transformadas. SI no hay que interpretarlas, me podria valer.
#Se podría sacar la relacion establecida en la transformacion con la rotacion:
PCA$rotation
#me indica los valores propios. Con PC2 tendríamos el 84% de la informacion.
#Con PC5 te quedas con la mitad de variables y tenemos el 95% de informacion de la varianza
#pero hay que ver el tradeoff, no hay nada que sea mejor o peor, todo dependerá de lo que queramos asumir.
#al final es un criterio, pero no hay un criterio.
plot(PCA)
#podemos guardar el modelo:
save(modelo10)
#podemos guardar el modelo:
save(modelo10,file="modelo10")
#una vez tengamos el modelo, puedo cargarlo directamtnet en la base de datos.( EN EL MOTOR DE LA BBDD, usando el lenguaje de la BBDD que se utilice) Ya que la mayor parte del tiempo se pierden en mover los datos.
#ANTES DE PONERNOS CON EL CODIGO, TENEMOS QUE TENER CLARO CUAL VA A SER LA LINEA DE PRODUCCION.
#MODELOS EN R QUE VAYAN A SISTEMA DE PRODUCCION NO HAY.
biplot(PCA,choices=c(1,3))
library(ggplot2)
library(effects)
install.packages("effects")
library(ggplot2)
library(effects)
library(plyr)
load("data/samsungData.rda")
load("data/samsungData.rda")
str(samsungData)
head(samsungData)
tail(samsungData)
View(samsungData)
View(samsungData)
table(samsungData$activity)
str(samsungData[,c(562,563)])
samsungScaled=scale(samsungData[,-c(562,563)])
set.seed(1234)
#hacemos y problemos con 6 clusters, luego con 8.
kClust1 <- kmeans(samsungScaled,centers=6)
table(kClust1$cluster,samsungData[,563])
#hacemos y problemos con 6 clusters, luego con 8.
# con k=6 el grupo 1 separa movimiento de no movimiento, el quinto grupo de pie. COn el grupo 2, al azar, si se le asigna laying aciertas en casi un 90% de los casos.
#El hacer el mismo nº de clsuters a categorias no es idoneo pq pueden no salirnos 6 grupos bien diferenciados
kClust1 <- kmeans(samsungScaled,centers=8)
table(kClust1$cluster,samsungData[,563])
set.seed(1234)
#hacemos y problemos con 6 clusters, luego con 8.
# con k=6 el grupo 1 separa movimiento de no movimiento, el quinto grupo de pie. COn el grupo 2, al azar, si se le asigna laying aciertas en casi un 90% de los casos.
#El hacer el mismo nº de clsuters a categorias no es idoneo pq pueden no salirnos 6 grupos bien diferenciados
kClust1 <- kmeans(samsungScaled,centers=8)
table(kClust1$cluster,samsungData[,563])
#vemos que al modelo le cuesta entre walk, walkup y woalkdown. SI tuviera que hacer un modelo predictivo lo haria sobre el grupo 8 y el 1.
nombres8<-c("walkup","laying","walkdown","laying","standing","sitting","laying","walkdown")
Error8=(length(samsungData[,563])-sum(nombres8[kClust1$cluster]==samsungData[,563]))/length(samsungData[,563])
Error8
## CLuster con 10 centros.
set.seed(1234)
kClust1 <- kmeans(samsungScaled,centers=10)
table(kClust1$cluster,samsungData[,563])
#se puede mejorar o poner en el foco en el grupo 1, 4, 8 y 9
nombres10<-c("walkup","laying","walkdown","sitting","standing","laying","laying","walkdown","sitting","walkup")
Error10=(length(samsungData[,563])-sum(nombres10[kClust1$cluster]==samsungData[,563]))/length(samsungData[,563])
Error10
PCA<-prcomp(samsungData[,-c(562,563)],scale=TRUE)
#PCA$rotation
#attributes(PCA)
summary(PCA)
summary(PCA)
plot(PCA)
#PCA$rotation
#attributes(PCA)
summary(PCA)
PCA$x[,1:3]
#para tener un diagrama vistoso cogemos las 2 primeras cmponentes principales
dev.off()
par(mfrow=c(1,3))
plot(PCA$x[,c(1,2)],col=as.numeric(as.factor(samsungData[,563])))
plot(PCA$x[,c(2,3)],col=as.numeric(as.factor(samsungData[,563])))
plot(PCA$x[,c(1,3)],col=as.numeric(as.factor(samsungData[,563])))
par(mfrow=c(1,1))
plot(PCA$x[,c(1,2)],col=as.numeric(as.factor(samsungData[,563])))
samsungData$cluster=kClust1$cluster
samsungData[,c(563,564)]
table(kClust1$cluster,samsungData[,563])
samsungData$cluster=nombres10[kClust1$cluster]
samsungData[,c(563,564)]
table(samsungData$activity,samsungData$cluster)
