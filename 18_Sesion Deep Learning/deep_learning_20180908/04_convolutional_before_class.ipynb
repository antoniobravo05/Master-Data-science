{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"04_convolutional_before_class.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"Tgelvf1_mXmF","colab_type":"text"},"cell_type":"markdown","source":["# Deep Learning for Computer Vision using Convolutional Neural Networks\n","\n","With dense layers applied to images, we have learnt global patterns that can be exploited to make predictions. The main difference with **convolutional** layers, is that we will now learn local patterns. \n","\n","These local patterns have two important features:\n","* They are translation invariant. It does not matter where in the image we see the pattern, the layer will be able to capture it and exploit it. In contrast, for a dense layer, if the same local pattern appears in two different location in the image, it would interpret them as two different patterns.\n","* Convolutional layers can lear a spatial hierarchy of patterns. Imagine the problem of recognizing a *face*. A first layer would learn something about the *nose*, some other layers about the *eyes*, and so on. The aggregation \n","\n","![](./imgs/09_cnn_hierarchy.png)\n","\n","The input to a convolutional layer is a 3-D tensor: height, width and depth (the channels in the image). For RGB images, depth will be 3, for the *R*ed, *B*lue and *G*reen colors. For a black and white image, depth will be just 1.\n","\n","Each layer recognizes a patch (subset) of the image, with a specific pattern. When applied to the original input, the layer will filter the rest of the image, highlighting the pattern that has learnt. That is, the layer becomes a **features map**.\n","\n","## Convolution operation\n","\n","For a convolutional layer, we need to decide the size of the patches (commonly, 3x3), and the depth of the output of the feature map (it is no longer 3, and in fact, it will be a number larger than that -- 16, 32, 64). The output will be another tensor, that is the input to the next layers. These tensors will no longer be images; that is, 3D tensors with a depth of 3, etc. To transform the output into a spatial tensor, we can use padding (adding additional rows or columns).\n","\n","![](./imgs/10_convolution.png)\n","\n","### Padding\n","\n","The convolution operation will slide through the image, trying to cover different zones, to extract common patterns found in different locations. In the edges of the image, the layer will not be able to extract patches, because the regions will be smaller than the patch size. With padding, we make it possible for the layer to extract patches even in the edges of the image, thus using that part of the image too to try to identify a common pattern.\n","\n","###  Stride\n","\n","Another parameter that we must take into account is *striding*. The patches can overlap with other patches. The distance between two windows used to extract patches is called **stride**. For instance, with a patch size of 3x3, and a stride of 3, patches will not overlap. We will normally will try to avoid overlapping in the windows extracting the patches; unless that during the training and validation process, we need to change the parameters to obtain a better model.\n","\n","### Pooling\n","\n","In a network, we need to partially reduce the dimension of the data, layer after layer, so we can learn at the output layer a number, a vector of a certain size, etc. The convolution operation is in fact increasing the size of the output. How do we do reduce the size of the output learnt in each layer? By **pooling**.\n","\n","The most typical pooling operation is *max pooling*. For each patch learnt in the layer, we apply a window of 2x2 or less (smaller than the striding window), and then apply the max operation. For each 2x2 possible window, we keep the max in that window. This way, we are reducing the size of the patches, and the size of the output of the layer. By doing this reduction in the size of the output, we will also help the network to build a hierarchy of patterns.\n","\n","Other pooling operations are also possible: averaging, in different ways. For full details of what pooling operations are available in Keras see https://keras.io/layers/pooling/\n","\n","## Additional readings/videos\n","\n","* How CNNs work: https://www.youtube.com/watch?v=FmpDIaiMIeA\n","* Deep Learning demystified: http://brohrer.github.io/deep_learning_demystified.html\n","* Hot-dog? No hot-dog? http://mateos.io/blog/getting-some-hotdogs/"]},{"metadata":{"id":"NtjRK1XzmXmH","colab_type":"text"},"cell_type":"markdown","source":["# Common functions and download data\n","\n"]},{"metadata":{"id":"lHOFBgd7mgmQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":32},"outputId":"4f9a4f72-1cdc-45fd-fb46-60f3b2f0a543","executionInfo":{"status":"ok","timestamp":1537555838523,"user_tz":-120,"elapsed":854,"user":{"displayName":"Antonio Bravo Muñoz","photoUrl":"","userId":"01083771191048821860"}}},"cell_type":"code","source":["%pylab inline\n","plt.style.use('seaborn-talk')"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Populating the interactive namespace from numpy and matplotlib\n"],"name":"stdout"}]},{"metadata":{"id":"4f4K4QzNmg5b","colab_type":"code","colab":{}},"cell_type":"code","source":["def plot_metric(history, metric):\n","    history_dict = history.history\n","    values = history_dict[metric]\n","    if 'val_' + metric in history_dict.keys():  \n","        val_values = history_dict['val_' + metric]\n","\n","    epochs = range(1, len(values) + 1)\n","\n","    if 'val_' + metric in history_dict.keys():  \n","        plt.plot(epochs, val_values, label='Validation')\n","    plt.semilogy(epochs, values, label='Training')\n","\n","    if 'val_' + metric in history_dict.keys():  \n","        plt.title('Training and validation %s' % metric)\n","    else:\n","        plt.title('Training %s' % metric)\n","    plt.xlabel('Epochs')\n","    plt.ylabel(metric.capitalize())\n","    plt.legend()\n","    plt.grid()\n","\n","    plt.show()  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"GMyUGqnemkuJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":142},"outputId":"68eb2f86-33fd-42c6-d0b7-d3c577698737","executionInfo":{"status":"ok","timestamp":1537555844367,"user_tz":-120,"elapsed":5077,"user":{"displayName":"Antonio Bravo Muñoz","photoUrl":"","userId":"01083771191048821860"}}},"cell_type":"code","source":["!pip install keras"],"execution_count":41,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.1.6)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (0.19.1)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.14.5)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.11.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n"],"name":"stdout"}]},{"metadata":{"id":"bRylfr0RTkVu","colab_type":"code","colab":{}},"cell_type":"code","source":["#Lo siguiente es para coger los datos de Drive, para ejecutar los datos en el framework de colab"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Wm_hQY5dmm0s","colab_type":"code","colab":{}},"cell_type":"code","source":["# Install the PyDrive wrapper & import libraries.\n","# This only needs to be done once per notebook.\n","!pip install -U -q PyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# Authenticate and create the PyDrive client.\n","# This only needs to be done once per notebook.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","# Download a file based on its file ID.\n","#\n","# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NEfuwzZumpb4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":57},"outputId":"77d7b2f9-43ad-4872-cd64-2e203260ef5d","executionInfo":{"status":"ok","timestamp":1537555855528,"user_tz":-120,"elapsed":4577,"user":{"displayName":"Antonio Bravo Muñoz","photoUrl":"","userId":"01083771191048821860"}}},"cell_type":"code","source":["!ls"],"execution_count":44,"outputs":[{"output_type":"stream","text":["adc.json  dogs_cats  dogs_cats_small  sample_data  test1.zip  train.zip\n"],"name":"stdout"}]},{"metadata":{"id":"acvJEl3kUBe_","colab_type":"code","colab":{}},"cell_type":"code","source":["#Ya he podido autenticarme. Vemos que hay dos ficheros. Con el id de fichero nos bajamos las imagnees tageadas (labeladas)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"q82Bk-IKmq2Y","colab_type":"code","colab":{}},"cell_type":"code","source":["file_id = '1nL7cgXGkNGS79FORsrCfrfcpzrBtoX8K'\n","downloaded = drive.CreateFile({'id': file_id})\n","downloaded.GetContentFile(\"train.zip\")\n","\n","file_id = '1edO-psKzj7gpYgf5PcDKyPgbFFJN09D3'\n","downloaded = drive.CreateFile({'id': file_id})\n","downloaded.GetContentFile(\"test1.zip\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4PPbk1lhnRsY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":57},"outputId":"ab6875ad-dfab-4599-fa78-3e2c80c9d4a9","executionInfo":{"status":"ok","timestamp":1537555873845,"user_tz":-120,"elapsed":3705,"user":{"displayName":"Antonio Bravo Muñoz","photoUrl":"","userId":"01083771191048821860"}}},"cell_type":"code","source":["!ls"],"execution_count":47,"outputs":[{"output_type":"stream","text":["adc.json  dogs_cats  dogs_cats_small  sample_data  test1.zip  train.zip\n"],"name":"stdout"}]},{"metadata":{"id":"DZocewGNUReu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"outputId":"297a5300-19ce-448f-9734-19d24abfd781","executionInfo":{"status":"ok","timestamp":1537555877202,"user_tz":-120,"elapsed":3324,"user":{"displayName":"Antonio Bravo Muñoz","photoUrl":"","userId":"01083771191048821860"}}},"cell_type":"code","source":["!ls -hl"],"execution_count":48,"outputs":[{"output_type":"stream","text":["total 815M\n","-rw-r--r-- 1 root root 2.5K Sep 21 18:06 adc.json\n","drwxr-xr-x 4 root root 4.0K Sep 21 18:08 dogs_cats\n","drwxr-xr-x 5 root root 4.0K Sep 21 18:12 dogs_cats_small\n","drwxr-xr-x 2 root root 4.0K Sep 20 00:09 sample_data\n","-rw-r--r-- 1 root root 272M Sep 21 18:51 test1.zip\n","-rw-r--r-- 1 root root 544M Sep 21 18:51 train.zip\n"],"name":"stdout"}]},{"metadata":{"id":"wwxDIeHenVhf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":142},"outputId":"691b5e3c-ed62-4823-a96c-c4399039f97b","executionInfo":{"status":"ok","timestamp":1537555905406,"user_tz":-120,"elapsed":28126,"user":{"displayName":"Antonio Bravo Muñoz","photoUrl":"","userId":"01083771191048821860"}}},"cell_type":"code","source":["!mkdir dogs_cats\n","!cd dogs_cats && unzip -q ../train.zip\n","!cd dogs_cats && unzip -q ../test1.zip\n","!ls -hl dogs_cats\n","#los datos están clasificados por ficheros en directorios diferentes."],"execution_count":49,"outputs":[{"output_type":"stream","text":["mkdir: cannot create directory ‘dogs_cats’: File exists\n","replace train/cat.0.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n","replace test1/1.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n","total 1.1M\n","drwxr-xr-x 2 root root 288K Sep 20  2013 test1\n","drwxr-xr-x 2 root root 756K Sep 20  2013 train\n"],"name":"stdout"}]},{"metadata":{"id":"Qw8ybMwLnYHH","colab_type":"code","colab":{}},"cell_type":"code","source":["#código para crear directorios e ir copiando ficheros a diferentes ficheros (train, test, validation). Esto es así pq no vamos a poder importarlos todos a la vez, pq son muchos datos.\n","#USAREMOS LOS GENERADORES DE PYTHON. Se cogen batches, lee del disco (lento), hace las operaciones, guarda los resultados y desecha los datos iniciales para volver a leer del disco los siguientes datos.\n","#Vimos algo de esto, eran los chunks.\n","#ESTO ES LO NORMAL CUANDO SE TRATAN IMÁGENES EN DEEP LEARNING.\n","\n","original_dataset_dir=\"/content/dogs_cats/train/\"\n","\n","import os, shutil\n","\n","base_dir = \"/content/dogs_cats_small\"\n","\n","\n","train_dir = os.path.join(base_dir, \"train\")\n","validation_dir = os.path.join(base_dir, \"validation\")\n","test_dir = os.path.join(base_dir, \"test\")\n","\n","\n","train_cats_dir = os.path.join(train_dir, \"cats\")\n","train_dogs_dir = os.path.join(train_dir, \"dogs\")\n","\n","validation_dogs_dir = os.path.join(validation_dir, \"dogs\")\n","validation_cats_dir = os.path.join(validation_dir, \"cats\")\n","\n","test_dogs_dir = os.path.join(test_dir, \"dogs\")\n","test_cats_dir = os.path.join(test_dir, \"cats\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FiSwnS0koij0","colab_type":"code","colab":{}},"cell_type":"code","source":["#Creación de los directorios (vamos a coger un subconjuntos de datos, de ahí las carpetas small)\n","!rm -rf dogs_cats_small/\n","os.mkdir(base_dir)\n","os.mkdir(train_dir)\n","os.mkdir(validation_dir)\n","os.mkdir(test_dir)\n","os.mkdir(train_cats_dir)\n","os.mkdir(train_dogs_dir)\n","os.mkdir(validation_dogs_dir)\n","os.mkdir(validation_cats_dir)\n","os.mkdir(test_dogs_dir)\n","os.mkdir(test_cats_dir)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lVVMrL-on_Y5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":208},"outputId":"50a1babb-6b77-4476-be8e-5f19e3128a65","executionInfo":{"status":"ok","timestamp":1537555969844,"user_tz":-120,"elapsed":3693,"user":{"displayName":"Antonio Bravo Muñoz","photoUrl":"","userId":"01083771191048821860"}}},"cell_type":"code","source":["!find dogs_cats_small"],"execution_count":67,"outputs":[{"output_type":"stream","text":["dogs_cats_small\n","dogs_cats_small/test\n","dogs_cats_small/test/cats\n","dogs_cats_small/test/dogs\n","dogs_cats_small/train\n","dogs_cats_small/train/cats\n","dogs_cats_small/train/dogs\n","dogs_cats_small/validation\n","dogs_cats_small/validation/cats\n","dogs_cats_small/validation/dogs\n"],"name":"stdout"}]},{"metadata":{"id":"UtdYxWUFoCDN","colab_type":"code","colab":{}},"cell_type":"code","source":["#COPIAMOS LOS FICHEROS DE SU LOCALIZACION ORIGNIAL Y LOS METEMOS A LOS DIRECTORIOS ANTERIORES\n","#1000 IMAGENES PARA ENTRENAR, 500 PARA VALIDAR Y POCAS PARA TESTEAR. Son pocas imagnees para DL, deberían ser 10 veces más\n","\n","fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\n","for fname in fnames:                                                       \n","    src = os.path.join(original_dataset_dir, fname)                        \n","    dst = os.path.join(train_cats_dir, fname)                              \n","    shutil.copyfile(src, dst)                                              \n","\n","fnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]               \n","for fname in fnames:                                                       \n","    src = os.path.join(original_dataset_dir, fname)                        \n","    dst = os.path.join(validation_cats_dir, fname)                         \n","    shutil.copyfile(src, dst)                                              \n","\n","fnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]               \n","for fname in fnames:                                                       \n","    src = os.path.join(original_dataset_dir, fname)                        \n","    dst = os.path.join(test_cats_dir, fname)                               \n","    shutil.copyfile(src, dst)                                              \n","\n","fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]                     \n","for fname in fnames:                                                       \n","    src = os.path.join(original_dataset_dir, fname)                        \n","    dst = os.path.join(train_dogs_dir, fname)                              \n","    shutil.copyfile(src, dst)                                              \n","fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]               \n","for fname in fnames:                                                       \n","    src = os.path.join(original_dataset_dir, fname)                        \n","    dst = os.path.join(validation_dogs_dir, fname)                         \n","    shutil.copyfile(src, dst)                                              \n","\n","fnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]               \n","for fname in fnames:                                                       \n","    src = os.path.join(original_dataset_dir, fname)                        \n","    dst = os.path.join(test_dogs_dir, fname)                               \n","    shutil.copyfile(src, dst) "],"execution_count":0,"outputs":[]},{"metadata":{"id":"9utma06EoLs0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":208},"outputId":"949ae578-1530-458c-a148-42ff4a4ab29d","executionInfo":{"status":"ok","timestamp":1537555978001,"user_tz":-120,"elapsed":3935,"user":{"displayName":"Antonio Bravo Muñoz","photoUrl":"","userId":"01083771191048821860"}}},"cell_type":"code","source":["!find dogs_cats_small/ | head"],"execution_count":69,"outputs":[{"output_type":"stream","text":["dogs_cats_small/\n","dogs_cats_small/test\n","dogs_cats_small/test/cats\n","dogs_cats_small/test/cats/cat.1831.jpg\n","dogs_cats_small/test/cats/cat.1989.jpg\n","dogs_cats_small/test/cats/cat.1574.jpg\n","dogs_cats_small/test/cats/cat.1564.jpg\n","dogs_cats_small/test/cats/cat.1784.jpg\n","dogs_cats_small/test/cats/cat.1966.jpg\n","dogs_cats_small/test/cats/cat.1516.jpg\n"],"name":"stdout"}]},{"metadata":{"id":"P6O6PHuHque2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":263},"outputId":"69f1b1d3-4916-4f70-fbce-6e80d56490dd","executionInfo":{"status":"ok","timestamp":1537555980032,"user_tz":-120,"elapsed":1996,"user":{"displayName":"Antonio Bravo Muñoz","photoUrl":"","userId":"01083771191048821860"}}},"cell_type":"code","source":["# Check if we have a GPU\n","from tensorflow.python.client import device_lib\n","device_lib.list_local_devices()"],"execution_count":70,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[name: \"/device:CPU:0\"\n"," device_type: \"CPU\"\n"," memory_limit: 268435456\n"," locality {\n"," }\n"," incarnation: 471243910195302528, name: \"/device:GPU:0\"\n"," device_type: \"GPU\"\n"," memory_limit: 230686720\n"," locality {\n","   bus_id: 1\n","   links {\n","   }\n"," }\n"," incarnation: 4458045193438441487\n"," physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"]"]},"metadata":{"tags":[]},"execution_count":70}]},{"metadata":{"id":"6udnOjpjoPKp","colab_type":"text"},"cell_type":"markdown","source":["# Prepare data\n","\n","Because we are dealing with large images, we cannot just read them in a Numpy array. We will use generators, to consume the images as they are needed by the network."]},{"metadata":{"id":"yzMRtMsWoeGC","colab_type":"code","colab":{}},"cell_type":"code","source":["#generador para train, test y validación. \n","#Lo que espera keras es que train esté en un directorio en la que cada etiqueta esté en un subdirectorio con los de una clase, otro con los de otra y así. Tantas como clases haya.\n","\n","#importando el generador(no de imágenes, sino de python) de imagenes\n","from keras.preprocessing import image\n","\n","#Cuando no hay suficiente imágenes se lleva cabo un aumentado de datos (rotar, especular, y demas transdormaciones. Esto ayuda a las RN a aprender los patrones aunque dispongan de la imagen original y mejorar en la clasificacion)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AHZ-C1SPW4MO","colab_type":"code","colab":{}},"cell_type":"code","source":["train_datagen=image.ImageDataGenerator(rescale=1./255.0)#normalizamos apra que vayan de 0 a 1\n","test_datagen=image.ImageDataGenerator(rescale=1./255.0)#normalizamos apra que vayan de 0 a 1\n","validation_datagen=image.ImageDataGenerator(rescale=1./255.0)#normalizamos apra que vayan de 0 a 1"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZJD3TTGXW4H-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":65},"outputId":"42d2648d-0a7f-42ba-aa7a-ceb4a3aff2fe","executionInfo":{"status":"ok","timestamp":1537555990741,"user_tz":-120,"elapsed":1262,"user":{"displayName":"Antonio Bravo Muñoz","photoUrl":"","userId":"01083771191048821860"}}},"cell_type":"code","source":["train_gen=train_datagen.flow_from_directory(train_dir,\n","                                            target_size=(100,100),#redimensionado de las imagenes. Todas las im que metamos a la RN tienen que tener el mismo tamaño.\n","                                            batch_size=100,#tamaño del batch (cuando entrenemos no vamos a usar el batch, será una funcion de generación)\n","                                            class_mode='binary'\n",")\n","test_gen=test_datagen.flow_from_directory(test_dir,\n","                                            target_size=(100,100),#redimensionado de las imagenes. Todas las im que metamos a la RN tienen que tener el mismo tamaño.\n","                                            batch_size=100,#tamaño del batch (cuando entrenemos no vamos a usar el batch, será una funcion de generación)\n","                                            class_mode='binary'\n",")\n","validation_gen=validation_datagen.flow_from_directory(validation_dir,\n","                                            target_size=(100,100),#redimensionado de las imagenes. Todas las im que metamos a la RN tienen que tener el mismo tamaño.\n","                                            batch_size=100,#tamaño del batch (cuando entrenemos no vamos a usar el batch, será una funcion de generación)\n","                                            class_mode='binary'\n",")"],"execution_count":73,"outputs":[{"output_type":"stream","text":["Found 2000 images belonging to 2 classes.\n","Found 1000 images belonging to 2 classes.\n","Found 1000 images belonging to 2 classes.\n"],"name":"stdout"}]},{"metadata":{"id":"mNSSCRVzW4Dl","colab_type":"code","colab":{}},"cell_type":"code","source":["#nos dice que ha encontrado en cada directorio. Justo lo que habíamos puesto nosotros. Está bien.\n","\n","#ahora no tenemos que distinguir entre X e Y pq el generador ya contiene esa info. Solo usaremos el generador.\n","\n","#No necesitariamos ningun tipo de procesamiento de imagenes más. El generador ya lo hace todo.\n","\n","#Pasamos a construir el modelo."],"execution_count":0,"outputs":[]},{"metadata":{"id":"eCRwHTVSW3-m","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"Iy72GQm8W34A","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"IA7grluGW3wB","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"5U17E5g9W3lC","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"eGRCGie2o8xi","colab_type":"text"},"cell_type":"markdown","source":["# Build model"]},{"metadata":{"id":"3GBgeCKXSVQ6","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras import models\n","from keras import layers"],"execution_count":0,"outputs":[]},{"metadata":{"id":"u9839PArZs5j","colab_type":"code","colab":{}},"cell_type":"code","source":["m=models.Sequential()\n","m.add(layers.Conv2D(32,(3,3),input_shape=(100,100,3),activation='relu'))#Convolucion2D, con dimensiones adicionales como video usariamos las 3D). Necesita las dimensiones de salida que tendrá la imagen de salida (3x3)\n","#está demostrado que la secuencia convolucion poolin es lo que mejor resultado da\n","m.add(layers.MaxPooling2D((2,2)))\n","m.add(layers.Conv2D(64,(3,3),activation='relu'))#convolucion aumenta mientras que en la capa densa disminuye\n","m.add(layers.MaxPooling2D((2,2)))\n","#antes de la capa densa, el flatten\n","m.add(layers.Flatten())\n","m.add(layers.Dense(256,activation='relu'))\n","m.add(layers.Dense(64,activation='relu'))\n","m.add(layers.Dense(1,activation='sigmoid'))\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HIlnW7vUZs-t","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras import optimizers\n","from keras import losses\n","from keras import metrics"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pY1UGqW0Zs81","colab_type":"code","colab":{}},"cell_type":"code","source":["m.compile(optimizer=optimizers.rmsprop(),\n","         loss=losses.binary_crossentropy,\n","         metrics=[metrics.binary_accuracy]\n","         )\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CQoFSqWhb6yY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":2995},"outputId":"4a461165-1829-4bb1-bfa9-7bc5049d58fc","executionInfo":{"status":"error","timestamp":1537556013640,"user_tz":-120,"elapsed":12148,"user":{"displayName":"Antonio Bravo Muñoz","photoUrl":"","userId":"01083771191048821860"}}},"cell_type":"code","source":["h = m.fit_generator(train_gen,epochs=10,steps_per_epoch=10,\n","                   validation_data=validation_gen,validation_steps=5)#hay que indicar el nº de elementos que hay que pedirle al generador para cada época. Ajustaremos el step_per_epoch. De esta forma controlamos el batch_size\n","#usará 500 imágenes para hacer la validación en cada época."],"execution_count":78,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n"],"name":"stdout"},{"output_type":"error","ename":"ResourceExhaustedError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor of shape [256] and type float\n\t [[Node: dense_7/Const = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [256] values: 0 0 0...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-78-1b81eb85d2b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m h = m.fit_generator(train_gen,epochs=10,steps_per_epoch=10,\n\u001b[0;32m----> 2\u001b[0;31m                    validation_data=validation_gen,validation_steps=5)#hay que indicar el nº de elementos que hay que pedirle al generador para cada época. Ajustaremos el step_per_epoch. De esta forma controlamos el batch_size\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#usará 500 imágenes para hacer la validación en cada época.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1313\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1315\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2228\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2229\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2230\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2478\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2479\u001b[0m         \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2480\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m   2482\u001b[0m                               **self.session_kwargs)\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m     \u001b[0;31m# hack for list_devices() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;31m# list_devices() function is not available under tensorflow r1.3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor of shape [256] and type float\n\t [[Node: dense_7/Const = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [256] values: 0 0 0...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\nCaused by op 'dense_7/Const', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-75-388bbae7304c>\", line 9, in <module>\n    m.add(layers.Dense(256,activation='relu'))\n  File \"/usr/local/lib/python3.6/dist-packages/keras/models.py\", line 522, in add\n    output_tensor = layer(self.outputs[0])\n  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/topology.py\", line 592, in __call__\n    self.build(input_shapes[0])\n  File \"/usr/local/lib/python3.6/dist-packages/keras/layers/core.py\", line 870, in build\n    constraint=self.bias_constraint)\n  File \"/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/topology.py\", line 413, in add_weight\n    weight = K.variable(initializer(shape),\n  File \"/usr/local/lib/python3.6/dist-packages/keras/initializers.py\", line 38, in __call__\n    return K.constant(0, shape=shape, dtype=dtype)\n  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 424, in constant\n    return tf.constant(value, dtype=dtype, shape=shape, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 202, in constant\n    name=name).outputs[0]\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor of shape [256] and type float\n\t [[Node: dense_7/Const = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [256] values: 0 0 0...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n"]}]},{"metadata":{"id":"0KUhrNd8b63k","colab_type":"code","colab":{}},"cell_type":"code","source":["#volvemos a re-entrenar el modelo y regenero los datos"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DGSvmuadb61k","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":65},"outputId":"3a98c34e-47e0-4b53-86f2-0e84d96a5687","executionInfo":{"status":"ok","timestamp":1537556037367,"user_tz":-120,"elapsed":1564,"user":{"displayName":"Antonio Bravo Muñoz","photoUrl":"","userId":"01083771191048821860"}}},"cell_type":"code","source":["train_gen=train_datagen.flow_from_directory(train_dir,\n","                                            target_size=(100,100),#redimensionado de las imagenes. Todas las im que metamos a la RN tienen que tener el mismo tamaño.\n","                                            batch_size=100,#tamaño del batch (cuando entrenemos no vamos a usar el batch, será una funcion de generación)\n","                                            class_mode='binary'\n",")\n","test_gen=test_datagen.flow_from_directory(test_dir,\n","                                            target_size=(100,100),#redimensionado de las imagenes. Todas las im que metamos a la RN tienen que tener el mismo tamaño.\n","                                            batch_size=100,#tamaño del batch (cuando entrenemos no vamos a usar el batch, será una funcion de generación)\n","                                            class_mode='binary'\n",")\n","validation_gen=validation_datagen.flow_from_directory(validation_dir,\n","                                            target_size=(100,100),#redimensionado de las imagenes. Todas las im que metamos a la RN tienen que tener el mismo tamaño.\n","                                            batch_size=100,#tamaño del batch (cuando entrenemos no vamos a usar el batch, será una funcion de generación)\n","                                            class_mode='binary'\n",")"],"execution_count":79,"outputs":[{"output_type":"stream","text":["Found 2000 images belonging to 2 classes.\n","Found 1000 images belonging to 2 classes.\n","Found 1000 images belonging to 2 classes.\n"],"name":"stdout"}]},{"metadata":{"id":"YvB0OzE8dXXX","colab_type":"code","colab":{}},"cell_type":"code","source":["m.compile(optimizer=optimizers.rmsprop(),\n","         loss=losses.binary_crossentropy,\n","         metrics=[metrics.binary_accuracy]\n","         )\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vNIiCrmfdXVT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":3110},"outputId":"7dfd3d63-5803-4611-a137-0c4328f9fb3d","executionInfo":{"status":"error","timestamp":1537556055610,"user_tz":-120,"elapsed":11744,"user":{"displayName":"Antonio Bravo Muñoz","photoUrl":"","userId":"01083771191048821860"}}},"cell_type":"code","source":["h = m.fit_generator(train_gen,epochs=30,steps_per_epoch=20,\n","                   validation_data=validation_gen,validation_steps=10)"],"execution_count":81,"outputs":[{"output_type":"stream","text":["Epoch 1/30\n"],"name":"stdout"},{"output_type":"error","ename":"ResourceExhaustedError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor of shape [256] and type float\n\t [[Node: training_4/RMSprop/zeros_5 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [256] values: 0 0 0...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-81-a80260861d8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m h = m.fit_generator(train_gen,epochs=30,steps_per_epoch=20,\n\u001b[0;32m----> 2\u001b[0;31m                    validation_data=validation_gen,validation_steps=10)\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1313\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1315\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2228\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2229\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2230\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2478\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2479\u001b[0m         \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2480\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m   2482\u001b[0m                               **self.session_kwargs)\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m     \u001b[0;31m# hack for list_devices() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;31m# list_devices() function is not available under tensorflow r1.3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor of shape [256] and type float\n\t [[Node: training_4/RMSprop/zeros_5 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [256] values: 0 0 0...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\nCaused by op 'training_4/RMSprop/zeros_5', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-81-a80260861d8f>\", line 2, in <module>\n    validation_data=validation_gen,validation_steps=10)\n  File \"/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/keras/models.py\", line 1315, in fit_generator\n    initial_epoch=initial_epoch)\n  File \"/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 2080, in fit_generator\n    self._make_train_function()\n  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 992, in _make_train_function\n    loss=self.total_loss)\n  File \"/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/keras/optimizers.py\", line 245, in get_updates\n    accumulators = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n  File \"/usr/local/lib/python3.6/dist-packages/keras/optimizers.py\", line 245, in <listcomp>\n    accumulators = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 693, in zeros\n    v = tf.zeros(shape=shape, dtype=tf_dtype, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\", line 1539, in zeros\n    output = _constant_if_small(zero, shape, dtype, name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\", line 1497, in _constant_if_small\n    return constant(value, shape=shape, dtype=dtype, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 202, in constant\n    name=name).outputs[0]\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor of shape [256] and type float\n\t [[Node: training_4/RMSprop/zeros_5 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [256] values: 0 0 0...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n"]}]},{"metadata":{"id":"kgOU5954dXTH","colab_type":"code","colab":{}},"cell_type":"code","source":["#plotear"],"execution_count":0,"outputs":[]},{"metadata":{"id":"u87rd3rjdXPf","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"7WQc8lUWtCyR","colab_type":"text"},"cell_type":"markdown","source":["# Evaluate model\n","\n","Our model is a binary classifier. We can evaluate it as any other classifier.\n","\n","**EXERCISE 1**. Obtain the confusion matrix and associated metrics (precision, recall, F-score) for this classifier.\n","\n","**EXERCISE 2**. Plot the ROC curve and calculate the AUC score.\n","\n","**EXERCISE 3**. What is the best model you can obtain using the above evaluation parameters?\n","\n","\n","In addition to this evaluation, for convolutional layers, we can attempt to plot each layer, applied to a image, to see what are the elements used by the model to find out the class the item belongs to."]},{"metadata":{"id":"D4yuhpnJSVQ8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":32},"outputId":"8575d238-1783-4780-aa4f-921dc01f8157","executionInfo":{"status":"ok","timestamp":1537556145770,"user_tz":-120,"elapsed":660,"user":{"displayName":"Antonio Bravo Muñoz","photoUrl":"","userId":"01083771191048821860"}}},"cell_type":"code","source":["N=18\n","test_gen[0][0].shape\n","#100 imágenes de 100X100 con colores RGB(3)"],"execution_count":82,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(100, 100, 100, 3)"]},"metadata":{"tags":[]},"execution_count":82}]},{"metadata":{"id":"XbDrvtW8fq3m","colab_type":"code","colab":{}},"cell_type":"code","source":["#prediccion\n","m.predict(test_gen[0][0][N:N+1])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"x4sVMEcDf8YT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":823},"outputId":"15165720-cb2c-4742-d9cc-4d917606a199","executionInfo":{"status":"ok","timestamp":1537556390287,"user_tz":-120,"elapsed":992,"user":{"displayName":"Antonio Bravo Muñoz","photoUrl":"","userId":"01083771191048821860"}}},"cell_type":"code","source":["test_gen[0][0][N]"],"execution_count":85,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[0.28235295, 0.25490198, 0.18431373],\n","        [0.43921572, 0.41176474, 0.34117648],\n","        [0.4156863 , 0.38823533, 0.31764707],\n","        ...,\n","        [0.        , 0.2509804 , 0.6117647 ],\n","        [0.01960784, 0.26666668, 0.6313726 ],\n","        [0.01568628, 0.25882354, 0.6       ]],\n","\n","       [[0.30980393, 0.28235295, 0.21176472],\n","        [0.42352945, 0.39607847, 0.3254902 ],\n","        [0.35686275, 0.32941177, 0.25882354],\n","        ...,\n","        [0.        , 0.25490198, 0.6156863 ],\n","        [0.01960784, 0.26666668, 0.6313726 ],\n","        [0.00392157, 0.24313727, 0.5921569 ]],\n","\n","       [[0.35686275, 0.32941177, 0.25882354],\n","        [0.41960788, 0.3921569 , 0.32156864],\n","        [0.3372549 , 0.30980393, 0.2392157 ],\n","        ...,\n","        [0.        , 0.25882354, 0.627451  ],\n","        [0.01568628, 0.27058825, 0.6392157 ],\n","        [0.00784314, 0.25882354, 0.6039216 ]],\n","\n","       ...,\n","\n","       [[0.56078434, 0.427451  , 0.32156864],\n","        [0.49803925, 0.3647059 , 0.26666668],\n","        [0.54901963, 0.41176474, 0.3254902 ],\n","        ...,\n","        [0.        , 0.2392157 , 0.627451  ],\n","        [0.00784314, 0.22352943, 0.61960787],\n","        [0.01960784, 0.2509804 , 0.6509804 ]],\n","\n","       [[0.5921569 , 0.45882356, 0.3529412 ],\n","        [0.5372549 , 0.4039216 , 0.30588236],\n","        [0.53333336, 0.39607847, 0.30980393],\n","        ...,\n","        [0.        , 0.2392157 , 0.627451  ],\n","        [0.00784314, 0.22352943, 0.61960787],\n","        [0.01176471, 0.25490198, 0.6509804 ]],\n","\n","       [[0.6745098 , 0.53333336, 0.43137258],\n","        [0.6313726 , 0.4901961 , 0.38823533],\n","        [0.60784316, 0.4666667 , 0.3647059 ],\n","        ...,\n","        [0.00784314, 0.27450982, 0.67058825],\n","        [0.        , 0.23137257, 0.60784316],\n","        [0.05882353, 0.3019608 , 0.6901961 ]]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":85}]},{"metadata":{"id":"vLCvC0jw__3r","colab_type":"text"},"cell_type":"markdown","source":["# Data augmentation\n","\n","If we don't have enough images to train our model, we can manipulate our images to produce modifications, and to *augment* the training data.\n","\n","Because images are slightly different, this can help the network to learn some of the patterns better.\n","\n","See https://keras.io/preprocessing/image/\n"]},{"metadata":{"id":"jeg4jcgsSVQ_","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"5_Q3QhtGxy5Z","colab_type":"text"},"cell_type":"markdown","source":["# Reusing a pre-trained convnet\n","\n","Training a convolutional network is slow and tedious. And if we think of every day objects, some patterns will probably be useful for images of different types.\n","\n","Similarly to word2vec, Glove, and other pre-trained word embeddings, we can use pre-trained convolutional networks, to improve our models."]},{"metadata":{"id":"OiHYp9ptC7sF","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"HQo9VQoUSVRE","colab_type":"text"},"cell_type":"markdown","source":["# Exercise\n","\n","In our classification problem, dogs are the *positive* case, and cats the *negative* (it could not be otherwise...).\n","\n","Obtain the ROC curve for the following classifiers:\n"," \n","* Dense network\n","* Convolutional layers\n","* Convolutional layers with training data augmentation\n","* Convolutional layers, using a pre-trained network, letting your network modify the weights\n","* Convolutional layers, using a pre-trained network, with all the weights frozen\n","\n","Which one is the best classifier? Why?"]},{"metadata":{"id":"Am9xkQ8fSVRF","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}