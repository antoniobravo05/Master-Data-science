library(tidyverse)
library(tmap)
crimes.houston <- crime %>% filter(!crime$offense %in% c("auto theft", "theft", "burglary"))
library(ggmap)
crimes.houston <- crime %>% filter(!crime$offense %in% c("auto theft", "theft", "burglary"))
#houston<-geocode('houston')
houston<-c(lon=-95.3698,lat= 29.76043)
HoustonMap <- ggmap(get_map(houston, zoom = 14, color = "bw"))
HoustonMap +
geom_point(aes(x = lon, y = lat, colour = offense), data = crimes.houston, size = 1)
HoustonMap +
geom_point(aes(x = lon, y = lat, colour = offense), data = crimes.houston, size = 1)+
facet_wrap(offense)
HoustonMap +
geom_point(aes(x = lon, y = lat, colour = offense), data = crimes.houston, size = 1)+
facet_wrap(~offense)
#EJERCICIO CON BASE DE DATOS DE CARBURANTES
carburante<-read_csv2("_book/data/carburantes.csv")
carburante
HoustonMap +
stat_density2d(aes(x = lon, y = lat, alpha = ..level..),fill="red4",
size = 2, data = subset(crimes.houston,offense=="robbery"),
geom = "polygon")
HoustonMap +
stat_density2d(aes(x = lon, y = lat, alpha = ..level..),fill="red4",
size = 2, data = crimes.houston,
geom = "polygon")+facet_wrap(~offense)
#Para que el color cambie el con el tipo de crimen hay
HoustonMap +
stat_density2d(aes(x = lon, y = lat, alpha = ..level..),fill=offense,
size = 2, data = crimes.houston,
geom = "polygon")+facet_wrap(~offense)
#Para que el color cambie el con el tipo de crimen hay
HoustonMap +
stat_density2d(aes(x = lon, y = lat, alpha = ..level..,fill=offense),
size = 2, data = crimes.houston,
geom = "polygon")+facet_wrap(~offense)
#Cuando los datos se presentan como datos acumulados (que no son tan precisos) puedo representarlos a través de áreas o polígonos.
install.packages("raster")
library(raster)
shape <- getData("GADM", country= "Spain", level = 2) #mapa administrativo a nivel provincial
plot(shape)
summary(shape)
head(shape)
unique(shape$NAME_0)
unique(shape$NAME_1)
#filtrando por islas canarias:
peninsula<-subset(shape,!NAME1=="Islas Canarias")
#Mapa sin las islas canarias:
peninsula<-subset(shape,!NAME_1=="Islas Canarias")
plot(peninsula)
#Mapa sin las islas canarias:
peninsula_only<-subset(shape,!NAME_1=="Islas Canarias")
plot(peninsula_only)
#podemos convertir el objeto complejo "shape" en uno mas amigable como son los DF.
peninsula.df=fortify(peninsula_only,region="CCA_2") #convierte el shape en data.frames
#podemos convertir el objeto complejo "shape" en uno mas amigable como son los DF.
peninsula.df=fortify(peninsula,region="CCA_2")
#Mapa sin las islas canarias:
peninsula<-subset(shape,!NAME_1=="Islas Canarias")
#podemos convertir el objeto complejo "shape" en uno mas amigable como son los DF.
peninsula.df=fortify(peninsula,region="CCA_2")
library(maptools)
install.packages('rgeos', type='source')
install.packages('rgdal', type='source')
library(maptools)
#podemos convertir el objeto complejo "shape" en uno mas amigable como son los DF.
peninsula.df=fortify(peninsula,region="CCA_2")
install.packages("gpclib", type="source")
library(gpclib)
#podemos convertir el objeto complejo "shape" en uno mas amigable como son los DF.
peninsula.df=fortify(peninsula,region="CCA_2")
library('maptools')
library(maptools)
#podemos convertir el objeto complejo "shape" en uno mas amigable como son los DF.
peninsula.df=fortify(peninsula,region="CCA_2")
ggplot(peninsula) + geom_polygon(data = peninsula.df, aes(long, lat, group = group))
ggplot(peninsula) + geom_polygon(aes(x=long, y=lat, group = group))
#podemos convertir el objeto complejo "shape" en uno mas amigable como son los DF.
peninsula.df=fortify(peninsula)
dim(peninsula.df)
head(peninsula.df)
unique(peninsula.df$id)
peninsula.df=fortify(peninsula,region="CCA_2")
peninsula.df=fortify(peninsula,region=CCA_2)
#Ahora tenemos que ver los datos de paro. La idea es preservar la integridad de los datos del mapa para luego poder fusionarlos con los del paro
paro<-read_csv2("_book/data/paro.csv")
paro
#Ahora tenemos que ver los datos de paro. La idea es preservar la integridad de los datos del mapa para luego poder fusionarlos con los del paro
paro<-read_csv("_book/data/paro.csv")
paro
#Para juntar las bases de datos usaremos funciones como el merge o el inner join
todo=inner_join(paro,peninsula.df,by=c("Prov.id=id"))
#Para juntar las bases de datos usaremos funciones como el merge o el inner join
todo=inner_join(paro,peninsula.df,by=c("Prov.id"="id"))
#Para juntar las bases de datos usaremos funciones como el merge o el inner join
peninsula.paro=inner_join(paro,peninsula.df,by=c("Prov.id"="id"))
dim(peninsula.paro)
View(peninsula_only)
rm(carburante,crimes.houston,todo)
head(peninsula.paro)
#representamos
ggplot(subset(peninsula.paro,Año==2011))+geom_polygon(x=long,y=lat,fill=Tasa.paro,group=group)+face_grid(Sexo~Trimestre)
subset(peninsula.paro,Año==2011)
#representamos
ggplot(subset(peninsula.paro,Año==2011))+geom_polygon(x=long,y=lat,fill=Tasa.paro,group=group)+face_grid(Sexo~Trimestre)
#representamos
ggplot(subset(peninsula.paro,Año==2011))+geom_polygon(long,lat,fill=Tasa.paro,group=group)+face_grid(Sexo~Trimestre)
#representamos
ggplot(subset(peninsula.paro,Año==2011))+geom_polygon(aes(long,lat,fill=Tasa.paro,group=group))+face_grid(Sexo~Trimestre)
#representamos
ggplot(subset(peninsula.paro,Año==2011))+geom_polygon(data=peninsula.paro,aes(long,lat,fill=Tasa.paro,group=group))+face_grid(Sexo~Trimestre)
#representamos
ggplot(subset(peninsula.paro,Año==2011))+geom_polygon(data=peninsula.paro,aes(long,lat,fill=Tasa.paro,group=group))+
facet_grid(Sexo~Trimestre)
#representamos
ggplot(subset(peninsula.paro,Año==2011))+geom_polygon(data=peninsula.paro,aes(long,lat,fill=Tasa.paro,group=group))+
facet_grid(Sexo~Trimestre)+ scale_fill_gradient(low="aliceblue",high="steelblue4")+coord_quickmap()
#representamos
ggplot(subset(peninsula.paro,Año==2011))+geom_polygon(data=peninsula.paro,aes(long,lat,fill=Tasa.paro,group=group))+
facet_grid(Sexo)+ scale_fill_gradient(low="aliceblue",high="steelblue4")+coord_quickmap()
#representamos
ggplot(subset(peninsula.paro,Año==2011))+geom_polygon(data=peninsula.paro,aes(long,lat,fill=Tasa.paro,group=group))+colour = "grey80", size = .1) +
facet_grid(~ Sexo) + scale_fill_gradient(low="aliceblue",high="steelblue4")+coord_quickmap()
#representamos
ggplot(subset(peninsula.paro,Año==2011))+geom_polygon(data=peninsula.paro,aes(long,lat,fill=Tasa.paro,group=group))+
facet_grid(Sexo~Trimestre)+ scale_fill_gradient(low="aliceblue",high="steelblue4")+coord_quickmap()
p<-ggplot(paro, aes(x = Año, y = Tasa.paro, color=Sexo,label=Provincia)) +
geom_jitter(alpha=.1) + geom_smooth(se=FALSE,method="lm")
ggplotly(p,tooltip = c("label", "color"))
plotly(p,tooltip = c("label", "color"))
install.packages("plotly")
library(plotly)
ggplotly(p,tooltip = c("label", "color"))
#otra opcion es tmap
library(leaflet)
library(tmap)
install.packages('tmap')
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
```{r pressure, echo=FALSE}
plot(pressure)
1:10
shiny::runApp('Rmarkdown y Shiny/Shiny/reactividad')
runApp('Rmarkdown y Shiny/Shiny/guardar')
runApp('Rmarkdown y Shiny/Shiny/guardar')
shiny::runApp('Rmarkdown y Shiny/Shiny/EjercicioPropuestoShiny')
runApp('Rmarkdown y Shiny/Shiny/EjercicioPropuestoShiny')
runApp('Rmarkdown y Shiny/Shiny/EjercicioPropuestoShiny')
runApp('Rmarkdown y Shiny/Shiny/EjercicioPropuestoShiny')
runApp('Rmarkdown y Shiny/Shiny/EjercicioPropuestoShiny')
runApp('Rmarkdown y Shiny/Shiny/EjercicioPropuestoShiny')
runApp('Rmarkdown y Shiny/Shiny/EjercicioPropuestoShiny')
runApp('Rmarkdown y Shiny/Shiny/EjercicioPropuestoShiny')
runApp('Rmarkdown y Shiny/Shiny/EjercicioPropuestoShiny')
runApp('Rmarkdown y Shiny/Shiny/EjercicioPropuestoShiny')
runApp('Rmarkdown y Shiny/Shiny/EjercicioPropuestoShiny')
amanece <- read_html("http://www.imdb.com/title/tt0094641/")
library(rvest)
?read_html
amanece <- read_html("http://www.imdb.com/title/tt0094641/")
amanece
amanece %>% html_nodes('title')
#Para extraer el texto: html_text
amanece %>% html_nodes('title') %>% html_text()
#Vemos los párrafos que hay
amanece %>% html_nodes('p')
#Si me interesa el párrafo 19:
amanece %>% html_nodes('p') %>% .[19]
amanece %>% html_nodes('p') %>% .[19] %>% html_text()
#Mirando la ayuda de HTML_text vemos la opcion trim, que me elimina lo que hay antes y despues del texto
amanece %>% html_nodes('p') %>% .[19] %>% html_text(trim=TRUE)
#EJERCICIO, DESCARGAR EL ULTIMO DISCURSO DEL REY DE ESPAÑA
discurso<-read_html("http://www.casareal.es/ES/Actividades/Paginas/actividades_discursos_detalle.aspx?data=5738")
discurso
discurso %>% html_nodes('title') %>% html_text()
discurso %>% html_nodes('p')
discurso %>% html_nodes('p') %>% .[1]
discurso %>% html_nodes('p') %>% .[1] %>% html_text(trim=TRUE)
discurso %>% html_nodes('p') %>% .[4] %>% html_text(trim=TRUE)
#EXTRACCION DE TABLAS.
p<-read_html("https://resultados.elpais.com/elecciones/2016/generales/congreso/")
p
#Tabla:Votos por partidos en Total España que es la segunda
tabla<-p %>% html_nodes("table") %>% .[2]
tabla
tabla %>% html_table() %>%
#Vemos las tablas que hay en amanece
amanece %>% html_nodes("table")
library(rvest)
amanece <- read_html("http://www.imdb.com/title/tt0094641/")
amanece %>% html_nodes("table")
tabla %>% html_table() %>%
#Vemos las tablas que hay en amanece y seleccionamos la primera para transdormarla en tipo DF
amanece %>% html_nodes("table") %>% .[1] %>% html_table(header = TRUE)
tabla %>% html_table() %>%
#Vemos las tablas que hay en amanece y seleccionamos la primera para transdormarla en tipo DF
amanece %>% html_nodes("table") %>% .[[1]] %>% html_table(header = TRUE)
View(amanece)
tabla %>% html_table() %>%
#Vemos las tablas que hay en amanece y seleccionamos la primera para transdormarla en tipo DF
amanece %>% html_nodes("table") %>% .[[1]] %>% html_table(header = TRUE)
tabla %>% html_table() %>%
#Vemos las tablas que hay en amanece y seleccionamos la primera para transdormarla en tipo DF
amanece %>% html_nodes("table") %>% .[1] %>% html_table(header = TRUE)
tabla %>% html_table()
#Vemos las tablas que hay en amanece y seleccionamos la primera para transdormarla en tipo DF
amanece %>% html_nodes("table") %>% .[1] %>% html_table(header = TRUE)
amanece %>% html_nodes("table") %>% .[1] %>% html_table(header = TRUE) %>% .[2]
amanece %>% html_nodes("table") %>% .[1] %>% html_table(header = TRUE) %>% .[[2]]
amanece %>% html_nodes("table") %>% .[[1]] %>% html_table(header = TRUE) %>% .[[2]]
#Extraccion de lo votos de la tabla del el Pais
tabla %>% html_table(header = TRUE)
#Extraccion de lo votos de la tabla del el Pais
tabla %>% html_table(header = TRUE)  %>% .[[3]]
#Extraccion de lo votos de la tabla del el Pais
tabla %>% html_table(header = TRUE)  %>% .[3]
#Extraccion de lo votos de la tabla del el Pais
tabla %>% html_table(header = TRUE)  %>% dim()
#Extraccion de lo votos de la tabla del el Pais
tabla %>% html_table(header = TRUE)
#Extraccion de lo votos de la tabla del el Pais
tabla<-p %>% html_nodes("table") %>% .[[2]]
tabla %>% html_table(header = TRUE)
tabla %>% html_table(header = TRUE) %>% .[[3]]
votos<-tabla %>% html_table(header = TRUE) %>% .[[3]]
votos
head(tabla)
#Extraccion de vínculos, util para ir de una web a otra.
#Le estoy diciendo, extrae todos los enlaces de esta tabla
amanece %>% html_nodes("table a")
#nos quedamos con el nombre
amanece %>% html_nodes("table a") %>% html_text(trim=TRUE)
#nos quedamos con el atributo correspondiente
amanece %>% html_nodes("table a") %>% html_attrs("href")
#nos quedamos con el atributo correspondiente
raiz<-"http://www.imdb.com"
jose=amanece %>% html_nodes("table a") %>% html_attr("href") %>% .[[1]]
jose
#Ahora concateno la raiz de la pagina con el resto de ruta
browseURL(paste(raiz,jose,sep="/"))
#Extraer la lista de provincias de la Wikipedia
#https://es.wikipedia.org/wiki/Provincia_de_Espa%C3%B1a
provincias<-read_html("https://es.wikipedia.org/wiki/Provincia_de_Espa%C3%B1a")
provincias
tablaprovincias<-provincias %>% html_nodes("table")
tablaprovincias
tablaprovincias %>% .[[1]]
tablaprovincias %>% .[[2]]
#Vemos que es la segunda tabla
tablaprovincias<-provincias %>% html_nodes("table") %>% .[[2]]
tablaprovincias
tablaprovincias %>% html_table(header = TRUE)
tablaprovincias %>% html_table()
tablaprovincias
dim(tablaprovincias)
View(tablaprovincias)
View(tablaprovincias)
#Vemos que es la segunda tabla
tablaprovincias<-provincias %>% html_nodes("table") %>% .[[2]]
tablaprovincias
tablaprovincias %>% html_table(fill=TRUE)
#Vemos que es la segunda tabla
tablaprovincias<-provincias %>% html_nodes("table") %>% .[[2]] %>% html_table(fill=TRUE)
tablaprovincias
listaprovincias<-tablaprovincias %>% .[[1]]
listaprovincias
#OTRO DISCURSO DEL REY PERO EN ESPAÑOL
discurso_ESP<-read_html("http://www.casareal.es/ES/Actividades/Paginas/actividades_discursos_detalle.aspx?data=5000")
discurso_ESP %>% html_nodes('p')
source('~/Desktop/DATA SCIENCE/Visualización de datos con R/scraping_sesion_antonio.R', echo=TRUE)
discurso_ESP %>% html_nodes('p') %>% html_text(trim=TRUE)
discurso_ESP %>% html_nodes('p') %>% html_text(trim=TRUE) %>% paste(collapse=" ")
#EXTRAER LAS COTIZACIONES EN BOLSA DEL IBEX
#“http://www.bolsamadrid.es/esp/aspx/Mercados/Precios.aspx?indice=ESI100000000”
cotizaciones<-read_html("http://www.bolsamadrid.es/esp/aspx/Mercados/Precios.aspx?indice=ESI100000000")
cotizaciones
cotizaciones %>% html_node('table')
cotizaciones %>% html_nodes('table')
#hay 5 tablas, vamos a checkear cual es
cotizaciones %>% html_nodes('table') %>% html_table(fill=TRUE) %>% names()
#hay 5 tablas, vamos a checkear cual es
cotizaciones %>% html_nodes('table') %>% html_table(fill=TRUE) %>% lapply(names)
cotizaciones %>% html_nodes('table') %>% html_table(fill=TRUE) %>% lapply(dim)
#Vemos que als tablas interesantes con la 4 y la 5.
#El ibex 35 tiene 35 empresas, luego será la última (35+1 filas)
datos_cotiz_ibex<-cotizaciones %>% html_nodes('table') %>% html_table(fill=TRUE,header=TRUE) %>% .[[5]]
datos_cotiz_ibex
sesion %>% jump_to("boxoffice")
#NAVEGACIÓN
sesion<-html_session("http://www.imdb.com")
sesion %>% jump_to("boxoffice")
sesion %>% jump_to("boxoffice") %>% html_nodes("table") %>% .[[1]] %>% html_table()
#-podemos ver el historial con session_history()
#-ir al 5 enlace de la pagina con: follow_link(5). SI hay un link que contiene Indian pues iremos a el con:
#follow_link('Indian')
#Extraccion de formularios
html_form(sesion)
#Seleccionamos el primero
html_form(sesion) %>% .[[1]]
#Seleccionamos el primero, fijo valores
busqueda<-html_form(sesion) %>% .[[1]] %>% set_values(q="sobrevivir",s="Titles")
busqueda
#Hago la solicitud a la página con el formulario relleno.
busqueda %>% submit_form(session=sesion) %>% html_nodes("table") %>% html_table()
#Vemos que es la primera table la que nos devuelve el resultado
busqueda %>% submit_form(session=sesion) %>% html_nodes("table") %>% html_table() %>% .[[1]]
#Limpieza:
gsup("h","H",c("hola","Búho"))
#que empieze por h
gsup("^h","H",c("hola","Búho")) #usamos expresiones regulares
#Limpieza:
gsub("h","H",c("hola","Búho"))
#que empieze por h
gsub("^h","H",c("hola","Búho")) #usamos expresiones regulares
str_replace(c("hola","Búho"),"^h","H")
#El patron siempre se considera como una funcino regular
#Podemos usar tambien str_replace()
library(tidyverse)
str_replace(c("hola","Búho"),"^h","H")
gsub("a","as",c("sandia","pera"))
gsub("a$","as",c("sandia","pera"))
gsub(".","",c("123.345","3.1416"))
gsub("//.","",c("123.345","3.1416"))
gsub("\\.","",c("123.345","3.1416"))
gsub(".","",c("123.345","3.1416"),fixed=TRUE)
#grep me permite detectar patrones
gsub("an",c("sandia","pera"))
#grep me permite detectar patrones
grep("an",c("sandia","pera"))
#grep me permite detectar patrones
grep("a",c("sandia","pera"))
#grep me permite detectar patrones
grep("a",c("sandia","pera",'platano'))
grep("a$",c("sandia","pera",'platano'),value=TRUE)
grep("blue$",colors(),value=TRUE)
paste("individuo",1:10,sep("-"))
paste("individuo",1:10,sep="-")
paste("fichero",Sys.Date(),".pdf",sep="_")
paste(c("Hoy es una buen dia","para salir"),collapse=" ")
#Separar de acuerdo a un criterio
strsplit("Hoy es una buen dia para salir",split=".",fixed=TRUE)
#Separar de acuerdo a un criterio
strsplit("Hoy es una buen dia para salir",split="/",fixed=TRUE)
#Separar de acuerdo a un criterio
strsplit("Hoy es una buen dia para salir",split="/"
#Separar de acuerdo a un criterio
strsplit("Hoy es una buen dia para salir",split="/"
)
#Separar de acuerdo a un criterio
strsplit("Hoy es una buen dia para salir",split="/")
#Separar de acuerdo a un criterio
strsplit("Hoy es una buen dia para salir",split=" ")
strsplit(fichero,"_\\d{5}_")
fichero<-c("ventas_pdf12345_pdf","ventas_segundo_23345_pdf")
strsplit(fichero,"_\\d{5}_")
fichero<-c("ventas_12345_pdf","ventas_segundo_23345_pdf")
strsplit(fichero,"_\\d{5}_")
strextract(fichero,"\\d{5}")
str_extract(fichero,"\\d{5}")
strsplit(fichero,"_\\d{5}_")
#EJERCICIO MEDALLA FIELD (PREMIO A LOS MEJORES MATEMÁTICOS MENORES DE 40 AÑOS)
require(rvest)
mfield<-read_html("https://es.wikipedia.org/w/index.php?title=Medalla_Fields&oldid=103644843")
mfield %>% html_nodes("table")
tabla <- mfield %>% html_nodes("table") %>% .[[2]] %>% html_table(header=TRUE)
knitr:::kable(tabla %>% head(10))
knitr:::kable(tabla %>% head(20))
knitr:::kable(tabla %>% head(-20))
tabla <- mfield %>% html_nodes("table") %>% .[[2]] %>% html_table(header=TRUE)
tabla
tabla <- mfield %>% html_nodes("table") %>% .[[2]] %>% html_table(header=TRUE) %>% .[[2]]
tabla
#Hay algunos premiados que tienen doble nacionalidad, extraemos la nacionalidad
paises<-str_extract(tabla,"\\([^()])\\")
#Hay algunos premiados que tienen doble nacionalidad, extraemos la nacionalidad
paises<-str_extract(tabla,"\\([^()]+\\]")
paises
#Hay algunos premiados que tienen doble nacionalidad, extraemos la nacionalidad
paises<-str_extract(tabla,"\\([^()]+\\)")
paises
#Ahora hay que quitar los parentesis
gsub("(","",paises)
#Ahora hay que quitar los parentesis
gsub("\\(","",paises)
#Ahora hay que quitar los parentesis
paises2<-gsub("\\(","",paises)
paises2<-gsub("\\)","",paises2)
paises2
#ó pasandole una lista de elementos en vez de hacerlo en 2 pasos
#paises2<-gsub("[()]","",paises2)
paises2 %>% strsplit(" y ") %>% unlist()
#ó pasandole una lista de elementos en vez de hacerlo en 2 pasos
#paises2<-gsub("[()]","",paises2)
paises2 %>% strsplit(" y ") %>% unlist() %>% str_replace("^ ","")
#ó pasandole una lista de elementos en vez de hacerlo en 2 pasos
#paises2<-gsub("[()]","",paises2)
paises2 %>% strsplit(" y ") %>% unlist() %>% str_replace("^[:blank:]","")
#ó pasandole una lista de elementos en vez de hacerlo en 2 pasos
#paises2<-gsub("[()]","",paises2)
listapaises<-paises2 %>% strsplit(" y ") %>% unlist() %>% str_replace("^[:blank:]","")
table(listapaises)
#Creacion del corpus (tokenizacion del texto)
texto<-c("Eso es insultar al lector, es llamarle torpe","Es decirle: ¡fíjate, hombre, fíjate, que aquí hay intención!","Y por eso le recomendaba yo a un señor que escribiese sus artículos todo en bastardilla","Para que el público se diese cuenta de que eran intencionadísimos desde la primera palabra a la última.")
texto
require(tidyverse)
texto_df <- data_frame(fila = 1:4, texto = texto)
texto_df
require(tidytext)
texto_df %>% unnest_tokens(palabra, texto)
texto_df %>% unnest_tokens(palabra, texto,token="ngrams",n=3)
texto_df %>% unnest_tokens(palabra, texto,token="ngrams",n=2)
require(janeaustenr)
austen_books()
libros <- austen_books()
libros
libros <- austen_books() %>%
group_by(book)
libros
libros <- austen_books() %>%
group_by(book)
libros
unique(libros$book)
libros <- austen_books() %>%
group_by(book) %>%
#Para cada libro defino unanueva columna con el nº de fila y la variable capítulo, para ello detecto el formato de los capítulos
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^chapter [[:digit:]ivxlc]", ignore_case=TRUE)))) %>%
ungroup()
libros
#ejemplo como funciona cumsum
cumsum(c(0,0,1,0,1,1,0))
tokens <- libros %>% unnest_tokens(word, text)
tokens
#Caracterizaremos cada novela mediante analisis de frecuencias de palabras.
#una palabra que caracteriza a una novela es la que aparece en la novela frecuentemente y es menos frecuente en el resto
#Se le da un peso a los tokens del corpus según aparece dentro de la novela y según aparece entre las novelas.
stop_words
require(stopwords)
stopwords(es)
stopwords(language="es")
stopwords(language="es",source="SMART")
stopwords(language="es")
#quitamos las palabras que no nos interesan
tokens<-tokens %>% anti_join(stop_words)
#Contamos las frecuencias
freq <- tokens %>% count(word, sort = TRUE)
freq
#representamos la distribución de las palabras más frecuentes:
require(ggplot2)
freq %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip()
require(wordcloud)
install.packages("wordcloud")
require(wordcloud)
wordcloud(words = freq$word, freq = freq$n, min.freq = 300,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
book_words <- austen_books() %>% unnest_tokens(word, text) %>%
count(book, word, sort = TRUE) %>%
ungroup()
freq_rel <- book_words %>% bind_tf_idf(word, book, n)
freq_rel
freq_rel %>% arrange(n)
freq_rel %>% arrange(idf,desc(idf))
freq_rel %>% arrange(desc(idf))
#vemos las palabras con mayor peso. Son nombres de personajes, como es lógico aparecen y son propios de cada novela.
freq_rel %>% arrange(desc(tf_idf))
freq_rel %>% arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>%
group_by(book) %>%
top_n(15) %>%
ungroup() %>%
ggplot(aes(word, tf_idf, fill = book)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~book, ncol = 2, scales = "free") +
coord_flip()
if(!require("ggplot2")){
install.packages("ggplot2")
library("ggplot2")
}
if (!require("gap")){
install.packages("gap")
library(gap)
}
setwd("/media/sf_Ubuntu_Data_Science/DATA SCIENCE MASTER VM/ Introducción a la modelización estadística. RegresionLineal")
creditos=read.csv("data/creditos.csv",stringsAsFactors = FALSE)
##### 3. Bloque de revisión basica del dataset #####
#cada registro es una persona con sus ingresos/hora en dólares. Balance es el saldo en cuenta.
str(creditos)
head(creditos)
