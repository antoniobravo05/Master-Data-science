{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSQL and DataFrames \n",
    "\n",
    "<a href = \"http://yogen.io\"><img src=\"http://yogen.io/assets/logo.svg\" alt=\"yogen\" style=\"width: 200px; float: right;\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDs, DataSets, and DataFrames\n",
    "\n",
    "RDDs are the original interface for Spark programming.\n",
    "\n",
    "DataFrames were introduced in 1.3\n",
    "\n",
    "Datasets were introduced in 1.6, and unified with DataFrames in 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of DataFrames:\n",
    "\n",
    "from https://www.datacamp.com/community/tutorials/apache-spark-python:\n",
    "\n",
    "> More specifically, the performance improvements are due to two things, which you’ll often come across when you’re reading up DataFrames: custom memory management (project Tungsten), which will make sure that your Spark jobs much faster given CPU constraints, and optimized execution plans (Catalyst optimizer), of which the logical plan of the DataFrame is a part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSQL and DataFrames \n",
    "\n",
    "\n",
    "pyspark does not have the Dataset API, which is available only if you use Spark from a statically typed language: Scala or Java.\n",
    "\n",
    "From https://spark.apache.org/docs/2.2.0/sql-programming-guide.html:\n",
    "\n",
    "> A DataFrame is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. The DataFrame API is available in Scala, Java, Python, and R. In Scala and Java, a DataFrame is represented by a Dataset of Rows. In the Scala API, DataFrame is simply a type alias of Dataset[Row]. While, in Java API, users need to use Dataset&lt;Row> to represent a DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The pyspark.sql module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important classes of Spark SQL and DataFrames:\n",
    "\n",
    "* `pyspark.sql.SparkSession` Main entry point for DataFrame and SQL functionality.\n",
    "\n",
    "* `pyspark.sql.DataFrame` A distributed collection of data grouped into named columns.\n",
    "\n",
    "* `pyspark.sql.Column` A column expression in a DataFrame.\n",
    "\n",
    "* `pyspark.sql.Row` A row of data in a DataFrame.\n",
    "\n",
    "* `pyspark.sql.GroupedData` Aggregation methods, returned by DataFrame.groupBy().\n",
    "\n",
    "* `pyspark.sql.DataFrameNaFunctions` Methods for handling missing data (null values).\n",
    "\n",
    "* `pyspark.sql.DataFrameStatFunctions` Methods for statistics functionality.\n",
    "\n",
    "* `pyspark.sql.functions` List of built-in functions available for DataFrame.\n",
    "\n",
    "* `pyspark.sql.types` List of data types available.\n",
    "\n",
    "* `pyspark.sql.Window` For working with window functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html\n",
    "\n",
    "https://spark.apache.org/docs/2.2.0/sql-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSession\n",
    "\n",
    "The traditional way to interact with Spark is the SparkContext. In the notebooks we get that from the pyspark driver.\n",
    "\n",
    "From 2.0 we can use SparkSession to replace SparkConf, SparkContext and SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.2.28.80:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "session = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the SparkSession there is a SpakContext that we can use to, for example, create RDDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.2.28.80:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passing other options to spark session:\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = SparkSession.builder\\\n",
    "                .config('someoption', 'somevalue')\\\n",
    "                .config('anotheroption', 'anothervalue')\\\n",
    "                .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check option values in the resulting session like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.id', 'local-1539960611002'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('anotheroption', 'anothervalue'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('someoption', 'somevalue'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.port', '33643'),\n",
       " ('spark.app.name', 'PySparkShell'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.host', '10.2.28.80')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrames\n",
    "\n",
    "SparkSession.createDataFrame: from an RDD, a list or a pandas.DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'psycopath'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.choice(['widgeteer', 'smoke salesman', 'wizard', 'psycopath'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['widgeteer',\n",
       " 'widgeteer',\n",
       " 'wizard',\n",
       " 'smoke salesman',\n",
       " 'smoke salesman',\n",
       " 'smoke salesman',\n",
       " 'widgeteer',\n",
       " 'widgeteer',\n",
       " 'psychopath',\n",
       " 'widgeteer',\n",
       " 'widgeteer',\n",
       " 'widgeteer',\n",
       " 'smoke salesman',\n",
       " 'smoke salesman',\n",
       " 'widgeteer',\n",
       " 'smoke salesman',\n",
       " 'psychopath',\n",
       " 'smoke salesman',\n",
       " 'psychopath',\n",
       " 'wizard']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "ids = range(20)\n",
    "positions = [random.choice(['widgeteer', 'smoke salesman', 'wizard', 'psychopath']) for id in ids ]\n",
    "positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data frame can be created like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: bigint, _2: string]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = zip(ids, positions)\n",
    "\n",
    "df = session.createDataFrame(rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it won't have informative field names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=0, _2='widgeteer'),\n",
       " Row(_1=1, _2='widgeteer'),\n",
       " Row(_1=2, _2='wizard'),\n",
       " Row(_1=3, _2='smoke salesman'),\n",
       " Row(_1=4, _2='smoke salesman')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| _1|            _2|\n",
      "+---+--------------+\n",
      "|  0|     widgeteer|\n",
      "|  1|     widgeteer|\n",
      "|  2|        wizard|\n",
      "|  3|smoke salesman|\n",
      "|  4|smoke salesman|\n",
      "+---+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Row in module pyspark.sql.types:\n",
      "\n",
      "class Row(builtins.tuple)\n",
      " |  A row in L{DataFrame}.\n",
      " |  The fields in it can be accessed:\n",
      " |  \n",
      " |  * like attributes (``row.key``)\n",
      " |  * like dictionary values (``row[key]``)\n",
      " |  \n",
      " |  ``key in row`` will search through row keys.\n",
      " |  \n",
      " |  Row can be used to create a row object by using named arguments,\n",
      " |  the fields will be sorted by names. It is not allowed to omit\n",
      " |  a named argument to represent the value is None or missing. This should be\n",
      " |  explicitly set to None in this case.\n",
      " |  \n",
      " |  >>> row = Row(name=\"Alice\", age=11)\n",
      " |  >>> row\n",
      " |  Row(age=11, name='Alice')\n",
      " |  >>> row['name'], row['age']\n",
      " |  ('Alice', 11)\n",
      " |  >>> row.name, row.age\n",
      " |  ('Alice', 11)\n",
      " |  >>> 'name' in row\n",
      " |  True\n",
      " |  >>> 'wrong_key' in row\n",
      " |  False\n",
      " |  \n",
      " |  Row also can be used to create another Row like class, then it\n",
      " |  could be used to create Row objects, such as\n",
      " |  \n",
      " |  >>> Person = Row(\"name\", \"age\")\n",
      " |  >>> Person\n",
      " |  <Row(name, age)>\n",
      " |  >>> 'name' in Person\n",
      " |  True\n",
      " |  >>> 'wrong_key' in Person\n",
      " |  False\n",
      " |  >>> Person(\"Alice\", 11)\n",
      " |  Row(name='Alice', age=11)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Row\n",
      " |      builtins.tuple\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, *args)\n",
      " |      create new Row object\n",
      " |  \n",
      " |  __contains__(self, item)\n",
      " |      Return key in self.\n",
      " |  \n",
      " |  __getattr__(self, item)\n",
      " |  \n",
      " |  __getitem__(self, item)\n",
      " |      Return self[key].\n",
      " |  \n",
      " |  __reduce__(self)\n",
      " |      Returns a tuple so Python knows how to pickle Row.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Printable representation of Row used in Python REPL.\n",
      " |  \n",
      " |  __setattr__(self, key, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  asDict(self, recursive=False)\n",
      " |      Return as an dict\n",
      " |      \n",
      " |      :param recursive: turns the nested Row as dict (default: False).\n",
      " |      \n",
      " |      >>> Row(name=\"Alice\", age=11).asDict() == {'name': 'Alice', 'age': 11}\n",
      " |      True\n",
      " |      >>> row = Row(key=1, value=Row(name='a', age=2))\n",
      " |      >>> row.asDict() == {'key': 1, 'value': Row(age=2, name='a')}\n",
      " |      True\n",
      " |      >>> row.asDict(True) == {'key': 1, 'value': {'name': 'a', 'age': 2}}\n",
      " |      True\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(self, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from builtins.tuple:\n",
      " |  \n",
      " |  __add__(self, value, /)\n",
      " |      Return self+value.\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __getnewargs__(...)\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __hash__(self, /)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __mul__(self, value, /)\n",
      " |      Return self*value.\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __rmul__(self, value, /)\n",
      " |      Return value*self.\n",
      " |  \n",
      " |  count(...)\n",
      " |      T.count(value) -> integer -- return number of occurrences of value\n",
      " |  \n",
      " |  index(...)\n",
      " |      T.index(value, [start, [stop]]) -> integer -- return first index of value.\n",
      " |      Raises ValueError if the value is not present.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "help(Row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(_1=0, _2='widgeteer')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = df.first()\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row._1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access fields in two ways, like in Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'widgeteer'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row._2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'widgeteer'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row['_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+\n",
      "|id_number|      position|\n",
      "+---------+--------------+\n",
      "|        0|     widgeteer|\n",
      "|        1|     widgeteer|\n",
      "|        2|        wizard|\n",
      "|        3|smoke salesman|\n",
      "|        4|smoke salesman|\n",
      "+---------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows = zip(ids, positions)\n",
    "df = session.createDataFrame(rows, schema=['id_number', 'position'])\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_number: long (nullable = true)\n",
      " |-- position: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[33] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id_number=0, position='widgeteer'),\n",
       " Row(id_number=1, position='widgeteer'),\n",
       " Row(id_number=2, position='wizard'),\n",
       " Row(id_number=3, position='smoke salesman'),\n",
       " Row(id_number=4, position='smoke salesman')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrames\n",
    "\n",
    "* From RDDs\n",
    "* from Hive tables\n",
    "* From Spark sources: parquet (default), json, jdbc, orc, libsvm, csv, text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coupon150720.csv MapPartitionsRDD[37] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = session.sparkContext.textFile('coupon150720.csv')\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['79062005698500,1,MAA,AUH,9W,9W,56.79,USD,1,H,H,0526,150904,OK,IAF0',\n",
       " '79062005698500,2,AUH,CDG,9W,9W,84.34,USD,1,H,H,6120,150905,OK,IAF0',\n",
       " '79062005924069,1,CJB,MAA,9W,9W,60.0,USD,1,H,H,2768,150721,OK,IAA0',\n",
       " '79065668570385,1,DEL,DXB,9W,9W,160.63,USD,2,S,S,0546,150804,OK,INA0',\n",
       " '79065668737021,1,AUH,IXE,9W,9W,152.46,USD,1,V,V,0501,150803,OK,INA0']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('79062005698500', 'MAA', 'AUH', '9W', 56.79),\n",
       " ('79062005698500', 'AUH', 'CDG', '9W', 84.34),\n",
       " ('79062005924069', 'CJB', 'MAA', '9W', 60.0),\n",
       " ('79065668570385', 'DEL', 'DXB', '9W', 160.63),\n",
       " ('79065668737021', 'AUH', 'IXE', '9W', 152.46)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse(line):\n",
    "\n",
    "    fields = line.split(',')\n",
    "    coupon = (fields[0], fields[2], fields[3], fields[4], float(fields[6]))\n",
    "    \n",
    "    return coupon # represented as a 5-element tuple\n",
    "\n",
    "coupons = lines.map(parse)\n",
    "\n",
    "coupons.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1232662"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.2.28.80:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8fa433e8d0>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From csv files\n",
    "\n",
    "We can either read them directly into dataframes or read them as RDDs and transform that into a DataFrame. This second way will be very useful if we have unstructured data like web server logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+---+---+---+---+------+---+---+---+----+----+------+----+----+\n",
      "|           _c0|_c1|_c2|_c3|_c4|_c5|   _c6|_c7|_c8|_c9|_c10|_c11|  _c12|_c13|_c14|\n",
      "+--------------+---+---+---+---+---+------+---+---+---+----+----+------+----+----+\n",
      "|79062005698500|  1|MAA|AUH| 9W| 9W| 56.79|USD|  1|  H|   H|0526|150904|  OK|IAF0|\n",
      "|79062005698500|  2|AUH|CDG| 9W| 9W| 84.34|USD|  1|  H|   H|6120|150905|  OK|IAF0|\n",
      "|79062005924069|  1|CJB|MAA| 9W| 9W|  60.0|USD|  1|  H|   H|2768|150721|  OK|IAA0|\n",
      "|79065668570385|  1|DEL|DXB| 9W| 9W|160.63|USD|  2|  S|   S|0546|150804|  OK|INA0|\n",
      "|79065668737021|  1|AUH|IXE| 9W| 9W|152.46|USD|  1|  V|   V|0501|150803|  OK|INA0|\n",
      "+--------------+---+---+---+---+---+------+---+---+---+----+----+------+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_csv = session.read.csv('coupon150720.csv')\n",
    "df_from_csv.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+---+---+---+---+------+---+---+---+----+----+------+----+----+\n",
      "|           _c0|_c1|_c2|_c3|_c4|_c5|   _c6|_c7|_c8|_c9|_c10|_c11|  _c12|_c13|_c14|\n",
      "+--------------+---+---+---+---+---+------+---+---+---+----+----+------+----+----+\n",
      "|79062005698500|  1|MAA|AUH| 9W| 9W| 56.79|USD|  1|  H|   H|0526|150904|  OK|IAF0|\n",
      "|79062005698500|  2|AUH|CDG| 9W| 9W| 84.34|USD|  1|  H|   H|6120|150905|  OK|IAF0|\n",
      "|79062005924069|  1|CJB|MAA| 9W| 9W|  60.0|USD|  1|  H|   H|2768|150721|  OK|IAA0|\n",
      "|79065668570385|  1|DEL|DXB| 9W| 9W|160.63|USD|  2|  S|   S|0546|150804|  OK|INA0|\n",
      "|79065668737021|  1|AUH|IXE| 9W| 9W|152.46|USD|  1|  V|   V|0501|150803|  OK|INA0|\n",
      "+--------------+---+---+---+---+---+------+---+---+---+----+----+------+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_csv = session.sql('SELECT * from csv.`coupon150720.csv`')\n",
    "df_from_csv.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+---+---+---+------+\n",
      "|           _c0|_c2|_c3|_c4|_c5|   _c6|\n",
      "+--------------+---+---+---+---+------+\n",
      "|79062005698500|MAA|AUH| 9W| 9W| 56.79|\n",
      "|79062005698500|AUH|CDG| 9W| 9W| 84.34|\n",
      "|79062005924069|CJB|MAA| 9W| 9W|  60.0|\n",
      "|79065668570385|DEL|DXB| 9W| 9W|160.63|\n",
      "|79065668737021|AUH|IXE| 9W| 9W|152.46|\n",
      "+--------------+---+---+---+---+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_csv = session.sql('SELECT _c0, _c2, _c3, _c4, _c5, _c6 from csv.`coupon150720.csv`')\n",
    "df_from_csv.show(5)\n",
    "df_from_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+---+---+---+------+\n",
      "|           _c0|_c2|_c3|_c4|_c5|   _c6|\n",
      "+--------------+---+---+---+---+------+\n",
      "|79062005698500|MAA|AUH| 9W| 9W| 56.79|\n",
      "|79062005698500|AUH|CDG| 9W| 9W| 84.34|\n",
      "|79062005924069|CJB|MAA| 9W| 9W|  60.0|\n",
      "|79065668570385|DEL|DXB| 9W| 9W|160.63|\n",
      "|79065668737021|AUH|IXE| 9W| 9W|152.46|\n",
      "+--------------+---+---+---+---+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_csv = session.sql('SELECT _c0, _c2, _c3, _c4, _c5, CAST(_c6 AS FLOAT)  from csv.`coupon150720.csv`')\n",
    "df_from_csv.show(5)\n",
    "df_from_csv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferring and specifying schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IntegerType"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "\n",
    "types.IntegerType()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fully specifying a schema\n",
    "\n",
    "We need to create a `StructType` composed of `StructField`s. each of those specifies afiled with name, type and `nullable` properties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_number: long (nullable = false)\n",
      " |-- position: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_schema = types.StructType([types.StructField('id_number', types.LongType(), nullable=False),\n",
    "                                    types.StructField('position', types.StringType(), nullable=True)])\n",
    "\n",
    "employees = session.createDataFrame(zip(ids, positions), schema=employee_schema) \n",
    "employees.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From other types of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Parquet is a free and open-source column-oriented data store of the Apache Hadoop ecosystem. It is similar to the other columnar storage file formats available in Hadoop namely RCFile and Optimized RCFile. It is compatible with most of the data processing frameworks in the Hadoop environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrameReader.jdbc of <pyspark.sql.readwriter.DataFrameReader object at 0x7f8f812d3f28>>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.read.parquet\n",
    "session.read.json\n",
    "session.read.jdbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic operations with DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+\n",
      "|id_number|      position|\n",
      "+---------+--------------+\n",
      "|        0|     widgeteer|\n",
      "|        1|     widgeteer|\n",
      "|        2|        wizard|\n",
      "|        3|smoke salesman|\n",
      "|        4|smoke salesman|\n",
      "+---------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employee_0 = employees.first()\n",
    "\n",
    "employee_0['id_number']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering and selecting\n",
    "\n",
    "Syntax inspired in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id_number: bigint]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employees.select('id_number')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to filter, we will need to build an instance of `Column`, using square bracket notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(employees['id_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+\n",
      "|id_number|      position|\n",
      "+---------+--------------+\n",
      "|        0|     widgeteer|\n",
      "|        1|     widgeteer|\n",
      "|        2|        wizard|\n",
      "|        3|smoke salesman|\n",
      "|        4|smoke salesman|\n",
      "+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.filter(employees['id_number'] < 5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's because a comparison between str and int will error out, so spark will not even get the chance to infer to which column we are referring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-b5d2a9419880>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memployees\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id_number'\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "employees.filter('id_number' < 5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`where` is exactly synonimous with `filter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+\n",
      "|id_number|      position|\n",
      "+---------+--------------+\n",
      "|        0|     widgeteer|\n",
      "|        1|     widgeteer|\n",
      "|        2|        wizard|\n",
      "|        3|smoke salesman|\n",
      "|        4|smoke salesman|\n",
      "+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.where(employees['id_number'] < 5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A column is quite different to a Pandas Series. It is just a reference to a column, and can only be used to construct sparkSQL expressions (select, where...). It can't be collected or taken as a one-dimensional sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-ad255f5b3e50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memployees\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id_number'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "col = employees['id_number']\n",
    "col.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Extract all employee ids which correspond to pyschopaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+\n",
      "|id_number|      position|\n",
      "+---------+--------------+\n",
      "|        0|     widgeteer|\n",
      "|        1|     widgeteer|\n",
      "|        2|        wizard|\n",
      "|        3|smoke salesman|\n",
      "|        4|smoke salesman|\n",
      "|        5|smoke salesman|\n",
      "|        6|     widgeteer|\n",
      "|        7|     widgeteer|\n",
      "|        8|    psychopath|\n",
      "|        9|     widgeteer|\n",
      "|       10|     widgeteer|\n",
      "|       11|     widgeteer|\n",
      "|       12|smoke salesman|\n",
      "|       13|smoke salesman|\n",
      "|       14|     widgeteer|\n",
      "|       15|smoke salesman|\n",
      "|       16|    psychopath|\n",
      "|       17|smoke salesman|\n",
      "|       18|    psychopath|\n",
      "|       19|        wizard|\n",
      "+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|id_number|\n",
      "+---------+\n",
      "|        8|\n",
      "|       16|\n",
      "|       18|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.filter(employees['position'] == 'psychopath').select('id_number').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding columns\n",
    "\n",
    "Dataframes are immutable, since they are built on top of RDDs, so we can not assign to them. We need to create new DataFrames with the appropriate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataFrame' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-62b65924c2e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memployees\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'salary'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memployees\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id_number'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'DataFrame' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "employees['salary'] = employees['id_number'] ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id_number: bigint, position: string, square: double, triquitri: bigint, trocotro: bigint]"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employees.withColumn('square', employees['id_number'] ** 2)\\\n",
    "         .withColumn('triquitri', employees['id_number'] % 2)\\\n",
    "         .withColumn('trocotro', employees['id_number'] % 2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id_number: bigint, position: string]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id_number: bigint, position: string, POWER(id_number, 2): double]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employees.select('id_number',\n",
    "                 'position',\n",
    "                 employees['id_number'] ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id_number: bigint, position: string, holi: double]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employees.select('id_number',\n",
    "                 'position',\n",
    "                 (employees['id_number'] ** 2).alias('holi'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User defined functions\n",
    "\n",
    "There are many useful functions in pyspark.sql.functions. These work on columns, that is, they are vectorial.\n",
    "\n",
    "We can write User Defined Functions (`udf`s), which allow us to \"vectorize\" operations: write a standard function to process single elements, then build a udf with that that works on columns in a DataFrame, like a SQL function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function log1p in module pyspark.sql.functions:\n",
      "\n",
      "log1p(col)\n",
      "    Computes the natural logarithm of the given value plus one.\n",
      "    \n",
      "    .. versionadded:: 1.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "\n",
    "help(functions.log1p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+------------------+\n",
      "|id_number|      position|  LOG1P(id_number)|\n",
      "+---------+--------------+------------------+\n",
      "|        0|     widgeteer|               0.0|\n",
      "|        1|     widgeteer|0.6931471805599453|\n",
      "|        2|        wizard|1.0986122886681096|\n",
      "|        3|smoke salesman|1.3862943611198906|\n",
      "|        4|smoke salesman|1.6094379124341003|\n",
      "+---------+--------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = employees.select('id_number',\n",
    "                      'position',\n",
    "                      functions.log1p(employees['id_number']))\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+------------------+\n",
      "|id_number|      position|  LOG1P(id_number)|\n",
      "+---------+--------------+------------------+\n",
      "|        0|     widgeteer|               0.0|\n",
      "|        1|     widgeteer|0.6931471805599453|\n",
      "|        2|        wizard|1.0986122886681096|\n",
      "|        3|smoke salesman|1.3862943611198906|\n",
      "|        4|smoke salesman|1.6094379124341003|\n",
      "+---------+--------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = employees.select('id_number',\n",
    "                      'position',\n",
    "                      functions.log1p('id_number'))\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "math.log1p(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be real number, not Column",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-1e41000aef8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m df = employees.select('id_number',\n\u001b[1;32m      2\u001b[0m                       \u001b[0;34m'position'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                       math.log1p(employees['id_number']))\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must be real number, not Column"
     ]
    }
   ],
   "source": [
    "df = employees.select('id_number',\n",
    "                      'position',\n",
    "                      math.log1p(employees['id_number']))\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This errors out because \n",
    "\n",
    "```python\n",
    "math.log1p\n",
    "```\n",
    "\n",
    "is not a udf: it doesn't know how to work with strings or Column objects:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can transform it into a udf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+------------------+\n",
      "|id_number|      position|  log1p(id_number)|\n",
      "+---------+--------------+------------------+\n",
      "|        0|     widgeteer|               0.0|\n",
      "|        1|     widgeteer|0.6931471805599453|\n",
      "|        2|        wizard|1.0986122886681096|\n",
      "|        3|smoke salesman|1.3862943611198906|\n",
      "|        4|smoke salesman|1.6094379124341003|\n",
      "+---------+--------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udf_log1p = functions.udf(math.log1p) \n",
    "\n",
    "df = employees.select('id_number',\n",
    "                      'position',\n",
    "                      udf_log1p(employees['id_number']))\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|      position|\n",
      "+--------------+\n",
      "|    psychopath|\n",
      "|     widgeteer|\n",
      "|smoke salesman|\n",
      "|        wizard|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('position').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same with any function we dream up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+---------+\n",
      "|id_number|      position|short_pos|\n",
      "+---------+--------------+---------+\n",
      "|        0|     widgeteer|      wid|\n",
      "|        1|     widgeteer|      wid|\n",
      "|        2|        wizard|      wiz|\n",
      "|        3|smoke salesman|      smo|\n",
      "|        4|smoke salesman|      smo|\n",
      "+---------+--------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shorten = functions.udf(lambda word: word[:3])\n",
    "\n",
    "df = employees.select('id_number',\n",
    "                      'position',\n",
    "                      shorten('position').alias('short_pos'))\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want the resulting columns to be of a particular type, we need to specify the return type. This is because in Python return types can not be inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_number: long (nullable = false)\n",
      " |-- position: string (nullable = true)\n",
      " |-- log1p(id_number): string (nullable = true)\n",
      "\n",
      "+---------+---------+------------------+\n",
      "|id_number| position|  log1p(id_number)|\n",
      "+---------+---------+------------------+\n",
      "|        0|widgeteer|               0.0|\n",
      "|        1|widgeteer|0.6931471805599453|\n",
      "+---------+---------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udf_log1p = functions.udf(math.log1p) \n",
    "\n",
    "df = employees.select('id_number',\n",
    "                      'position',\n",
    "                      udf_log1p(employees['id_number']))\n",
    "\n",
    "df.printSchema()\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringType"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udf_log1p.returnType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function udf in module pyspark.sql.functions:\n",
      "\n",
      "udf(f=None, returnType=StringType)\n",
      "    Creates a user defined function (UDF).\n",
      "    \n",
      "    .. note:: The user-defined functions are considered deterministic by default. Due to\n",
      "        optimization, duplicate invocations may be eliminated or the function may even be invoked\n",
      "        more times than it is present in the query. If your function is not deterministic, call\n",
      "        `asNondeterministic` on the user defined function. E.g.:\n",
      "    \n",
      "    >>> from pyspark.sql.types import IntegerType\n",
      "    >>> import random\n",
      "    >>> random_udf = udf(lambda: int(random.random() * 100), IntegerType()).asNondeterministic()\n",
      "    \n",
      "    .. note:: The user-defined functions do not support conditional expressions or short circuiting\n",
      "        in boolean expressions and it ends up with being executed all internally. If the functions\n",
      "        can fail on special rows, the workaround is to incorporate the condition into the functions.\n",
      "    \n",
      "    :param f: python function if used as a standalone function\n",
      "    :param returnType: the return type of the user-defined function. The value can be either a\n",
      "        :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "    \n",
      "    >>> from pyspark.sql.types import IntegerType\n",
      "    >>> slen = udf(lambda s: len(s), IntegerType())\n",
      "    >>> @udf\n",
      "    ... def to_upper(s):\n",
      "    ...     if s is not None:\n",
      "    ...         return s.upper()\n",
      "    ...\n",
      "    >>> @udf(returnType=IntegerType())\n",
      "    ... def add_one(x):\n",
      "    ...     if x is not None:\n",
      "    ...         return x + 1\n",
      "    ...\n",
      "    >>> df = spark.createDataFrame([(1, \"John Doe\", 21)], (\"id\", \"name\", \"age\"))\n",
      "    >>> df.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\")).show()\n",
      "    +----------+--------------+------------+\n",
      "    |slen(name)|to_upper(name)|add_one(age)|\n",
      "    +----------+--------------+------------+\n",
      "    |         8|      JOHN DOE|          22|\n",
      "    +----------+--------------+------------+\n",
      "    \n",
      "    .. versionadded:: 1.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(functions.udf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about this function: what is its return type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def anonymous(element):\n",
    "    result = element + element\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FloatType"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udf_log1p_2 = functions.udf(math.log1p, returnType=types.FloatType())\n",
    "udf_log1p_2.returnType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: \n",
    "\n",
    "Create a 'salary' field in our df. make it 100000 for psychopaths, 35000 for widgeteers, 50000 for smoke salesmen 60000 for wizards\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|      position|\n",
      "+--------------+\n",
      "|    psychopath|\n",
      "|     widgeteer|\n",
      "|smoke salesman|\n",
      "|        wizard|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('position').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def salary(position):\n",
    "\n",
    "    salary_tables = {'psychopath' : 100000, 'widgeteer' : 35000, 'smoke salesman' : 50000, 'wizard' : 60000 }\n",
    "    \n",
    "    return salary_tables[position]\n",
    "    \n",
    "salary('wizard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+----------------+\n",
      "|id_number|      position|salary(position)|\n",
      "+---------+--------------+----------------+\n",
      "|        0|     widgeteer|           35000|\n",
      "|        1|     widgeteer|           35000|\n",
      "|        2|        wizard|           60000|\n",
      "|        3|smoke salesman|           50000|\n",
      "|        4|smoke salesman|           50000|\n",
      "+---------+--------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salary_udf = functions.udf(salary) \n",
    "\n",
    "df = employees.select('id_number',\n",
    "                      'position',\n",
    "                      salary_udf(employees['position']))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_number: long (nullable = false)\n",
      " |-- position: string (nullable = true)\n",
      " |-- salary(position): string (nullable = true)\n",
      " |-- bonus: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('bonus', df['salary(position)'] * 0.1) \n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_number: long (nullable = false)\n",
      " |-- position: string (nullable = true)\n",
      " |-- salary(position): integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salary_udf = functions.udf(salary, returnType=types.IntegerType()) \n",
    "\n",
    "df = employees.select('id_number',\n",
    "                      'position',\n",
    "                      salary_udf(employees['position']))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a column that is not the desired type, we can convert it with `cast`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id_number: bigint, position: string, salary(position): int]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('id_number',\n",
    "          'position',\n",
    "          df['salary(position)'].cast(types.IntegerType())\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary statistics\n",
    "\n",
    "https://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38648340657639785"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.stat.corr('id_number', 'salary(position)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50789.47368421053"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.stat.cov('id_number', 'salary(position)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .crosstab()\n",
    "\n",
    "Crosstab returns the contingency table for two columns, as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_udf = functions.udf(lambda: random.choice(['Madrid', 'Barcelona']))\n",
    "\n",
    "with_locs = df.select('id_number',\n",
    "               'position',\n",
    "               df['salary(position)'].alias('salary'),\n",
    "               location_udf().alias('location'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+------+---------+\n",
      "|id_number|      position|salary| location|\n",
      "+---------+--------------+------+---------+\n",
      "|        0|     widgeteer| 35000|   Madrid|\n",
      "|        1|     widgeteer| 35000|Barcelona|\n",
      "|        2|        wizard| 60000|Barcelona|\n",
      "|        3|smoke salesman| 50000|   Madrid|\n",
      "|        4|smoke salesman| 50000|   Madrid|\n",
      "|        5|smoke salesman| 50000|Barcelona|\n",
      "|        6|     widgeteer| 35000|   Madrid|\n",
      "|        7|     widgeteer| 35000|Barcelona|\n",
      "|        8|    psychopath|100000|   Madrid|\n",
      "|        9|     widgeteer| 35000|Barcelona|\n",
      "|       10|     widgeteer| 35000|   Madrid|\n",
      "|       11|     widgeteer| 35000|Barcelona|\n",
      "|       12|smoke salesman| 50000|   Madrid|\n",
      "|       13|smoke salesman| 50000|Barcelona|\n",
      "|       14|     widgeteer| 35000|   Madrid|\n",
      "|       15|smoke salesman| 50000|Barcelona|\n",
      "|       16|    psychopath|100000|   Madrid|\n",
      "|       17|smoke salesman| 50000|Barcelona|\n",
      "|       18|    psychopath|100000|   Madrid|\n",
      "|       19|        wizard| 60000|Barcelona|\n",
      "+---------+--------------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with_locs.cache()\n",
    "with_locs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+------+\n",
      "|position_location|Barcelona|Madrid|\n",
      "+-----------------+---------+------+\n",
      "|   smoke salesman|        4|     3|\n",
      "|       psychopath|        0|     3|\n",
      "|           wizard|        2|     0|\n",
      "|        widgeteer|        4|     4|\n",
      "+-----------------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with_locs.crosstab('position', 'location').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping\n",
    "\n",
    "Grouping works very similarly to Pandas: executing groupby (or groupBy) on a DataFrame will return an object (a GroupedData) that can then be aggregated to obtain the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x7f8f8127ca20>"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb = with_locs.groupby('location')\n",
    "gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GroupedData has several aggregation functions defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+-----------+\n",
      "| location|avg(id_number)|avg(salary)|\n",
      "+---------+--------------+-----------+\n",
      "|   Madrid|           9.1|    59000.0|\n",
      "|Barcelona|           9.9|    46000.0|\n",
      "+---------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gb.avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "| location|avg(salary)|\n",
      "+---------+-----------+\n",
      "|   Madrid|    59000.0|\n",
      "|Barcelona|    46000.0|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gb.avg('salary').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do several aggregations in a single step, with a number of different syntaxes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+-----------+\n",
      "| location|max(id_number)|avg(salary)|\n",
      "+---------+--------------+-----------+\n",
      "|   Madrid|            18|    59000.0|\n",
      "|Barcelona|            19|    46000.0|\n",
      "+---------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gb.agg({'id_number': 'max', 'salary': 'mean' }).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------+------------------------+\n",
      "| location|avg(salary)|count(salary)|count(DISTINCT position)|\n",
      "+---------+-----------+-------------+------------------------+\n",
      "|   Madrid|    59000.0|           10|                       3|\n",
      "|Barcelona|    46000.0|           10|                       3|\n",
      "+---------+-----------+-------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = gb.agg(functions.mean('salary'),\n",
    "                functions.count('salary'),\n",
    "                functions.countDistinct('position'))\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersections\n",
    "\n",
    "Very much like SQL joins. We can specify the columns and the join method (left, right, inner, outer) or we can let Spark infer them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------------+\n",
      "| position|id_number|             raise|\n",
      "+---------+---------+------------------+\n",
      "|   Madrid|        4|297.97219438070346|\n",
      "|Barcelona|        3|2186.3797480360336|\n",
      "|Barcelona|       17| 5053.552881033624|\n",
      "|   Madrid|        2|265.35969683863624|\n",
      "|  Sevilla|       18| 1988.376506866485|\n",
      "|Barcelona|       13|6498.8443777952325|\n",
      "|Barcelona|        1| 5449.414806032167|\n",
      "+---------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "raises = session.createDataFrame(list(zip([random.choice(['Barcelona', 'Sevilla', 'Madrid']) for _ in range(7)],\n",
    "                                           [random.randint(0, 19) for _ in range(7)],\n",
    "                                           [random.random() * 10000 for _ in range(7)])),\n",
    "                                  schema= ['position', 'id_number', 'raise']).cache()\n",
    "raises.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Detected cartesian product for INNER join between logical plans\\nInMemoryRelation [id_number#612L, position#613, salary#1083, location#1084], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\\n   +- *(1) Project [id_number#612L, position#613, pythonUDF0#1267 AS salary#1083, pythonUDF1#1268 AS location#1084]\\n      +- BatchEvalPython [salary(position#613), <lambda>()], [id_number#612L, position#613, pythonUDF0#1267, pythonUDF1#1268]\\n         +- Scan ExistingRDD[id_number#612L,position#613]\\nand\\nInMemoryRelation [position#2243, id_number#2244L, raise#2245], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\\n   +- Scan ExistingRDD[position#2243,id_number#2244L,raise#2245]\\nJoin condition is missing or trivial.\\nUse the CROSS JOIN syntax to allow cartesian products between these relations.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/master-p3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2084.showString.\n: org.apache.spark.sql.AnalysisException: Detected cartesian product for INNER join between logical plans\nInMemoryRelation [id_number#612L, position#613, salary#1083, location#1084], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n   +- *(1) Project [id_number#612L, position#613, pythonUDF0#1267 AS salary#1083, pythonUDF1#1268 AS location#1084]\n      +- BatchEvalPython [salary(position#613), <lambda>()], [id_number#612L, position#613, pythonUDF0#1267, pythonUDF1#1268]\n         +- Scan ExistingRDD[id_number#612L,position#613]\nand\nInMemoryRelation [position#2243, id_number#2244L, raise#2245], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n   +- Scan ExistingRDD[position#2243,id_number#2244L,raise#2245]\nJoin condition is missing or trivial.\nUse the CROSS JOIN syntax to allow cartesian products between these relations.;\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$21.applyOrElse(Optimizer.scala:1124)\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$21.applyOrElse(Optimizer.scala:1121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:256)\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$.apply(Optimizer.scala:1121)\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$.apply(Optimizer.scala:1103)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)\n\tat scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:72)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:68)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3248)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2698)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n\tat sun.reflect.GeneratedMethodAccessor75.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-321-ad3f3cc9f987>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwith_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraises\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \"\"\"\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/master-p3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Detected cartesian product for INNER join between logical plans\\nInMemoryRelation [id_number#612L, position#613, salary#1083, location#1084], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\\n   +- *(1) Project [id_number#612L, position#613, pythonUDF0#1267 AS salary#1083, pythonUDF1#1268 AS location#1084]\\n      +- BatchEvalPython [salary(position#613), <lambda>()], [id_number#612L, position#613, pythonUDF0#1267, pythonUDF1#1268]\\n         +- Scan ExistingRDD[id_number#612L,position#613]\\nand\\nInMemoryRelation [position#2243, id_number#2244L, raise#2245], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\\n   +- Scan ExistingRDD[position#2243,id_number#2244L,raise#2245]\\nJoin condition is missing or trivial.\\nUse the CROSS JOIN syntax to allow cartesian products between these relations.;'"
     ]
    }
   ],
   "source": [
    "with_locs.join(raises).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark refuses to do cross joins by default. To perform them, we can \n",
    "\n",
    "a) Allow then explicitly:\n",
    "\n",
    "```python\n",
    "session.conf.set(\"spark.sql.crossJoin.enabled\", \"true\")\n",
    "```\n",
    "\n",
    "b) Specify the join criterion\n",
    "\n",
    "```python\n",
    "df4.join(new_df, on='id').show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+------+---------+---------+------------------+\n",
      "|id_number|      position|salary| location| position|             raise|\n",
      "+---------+--------------+------+---------+---------+------------------+\n",
      "|        1|     widgeteer| 35000|Barcelona|Barcelona| 5449.414806032167|\n",
      "|        2|        wizard| 60000|Barcelona|   Madrid|265.35969683863624|\n",
      "|        3|smoke salesman| 50000|   Madrid|Barcelona|2186.3797480360336|\n",
      "|        4|smoke salesman| 50000|   Madrid|   Madrid|297.97219438070346|\n",
      "|       13|smoke salesman| 50000|Barcelona|Barcelona|6498.8443777952325|\n",
      "|       17|smoke salesman| 50000|Barcelona|Barcelona| 5053.552881033624|\n",
      "|       18|    psychopath|100000|   Madrid|  Sevilla| 1988.376506866485|\n",
      "+---------+--------------+------+---------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with_locs.join(raises, on='id_number').show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+------+---------+---------+------------------+\n",
      "|id_number|      position|salary| location| position|             raise|\n",
      "+---------+--------------+------+---------+---------+------------------+\n",
      "|        0|     widgeteer| 35000|   Madrid|     null|              null|\n",
      "|        1|     widgeteer| 35000|Barcelona|Barcelona| 5449.414806032167|\n",
      "|        2|        wizard| 60000|Barcelona|   Madrid|265.35969683863624|\n",
      "|        3|smoke salesman| 50000|   Madrid|Barcelona|2186.3797480360336|\n",
      "|        4|smoke salesman| 50000|   Madrid|   Madrid|297.97219438070346|\n",
      "|        5|smoke salesman| 50000|Barcelona|     null|              null|\n",
      "|        6|     widgeteer| 35000|   Madrid|     null|              null|\n",
      "|        7|     widgeteer| 35000|Barcelona|     null|              null|\n",
      "|        8|    psychopath|100000|   Madrid|     null|              null|\n",
      "|        9|     widgeteer| 35000|Barcelona|     null|              null|\n",
      "|       10|     widgeteer| 35000|   Madrid|     null|              null|\n",
      "|       11|     widgeteer| 35000|Barcelona|     null|              null|\n",
      "|       12|smoke salesman| 50000|   Madrid|     null|              null|\n",
      "|       13|smoke salesman| 50000|Barcelona|Barcelona|6498.8443777952325|\n",
      "|       14|     widgeteer| 35000|   Madrid|     null|              null|\n",
      "|       15|smoke salesman| 50000|Barcelona|     null|              null|\n",
      "|       16|    psychopath|100000|   Madrid|     null|              null|\n",
      "|       17|smoke salesman| 50000|Barcelona|Barcelona| 5053.552881033624|\n",
      "|       18|    psychopath|100000|   Madrid|  Sevilla| 1988.376506866485|\n",
      "|       19|        wizard| 60000|Barcelona|     null|              null|\n",
      "+---------+--------------+------+---------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with_locs.join(raises, on='id_number',how='left').show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+------+---------+-----+\n",
      "|id_number|      position|salary| location|raise|\n",
      "+---------+--------------+------+---------+-----+\n",
      "|        0|     widgeteer| 35000|   Madrid| null|\n",
      "|        1|     widgeteer| 35000|Barcelona| null|\n",
      "|        2|        wizard| 60000|Barcelona| null|\n",
      "|        3|smoke salesman| 50000|   Madrid| null|\n",
      "|        4|smoke salesman| 50000|   Madrid| null|\n",
      "|        5|smoke salesman| 50000|Barcelona| null|\n",
      "|        6|     widgeteer| 35000|   Madrid| null|\n",
      "|        7|     widgeteer| 35000|Barcelona| null|\n",
      "|        8|    psychopath|100000|   Madrid| null|\n",
      "|        9|     widgeteer| 35000|Barcelona| null|\n",
      "|       10|     widgeteer| 35000|   Madrid| null|\n",
      "|       11|     widgeteer| 35000|Barcelona| null|\n",
      "|       12|smoke salesman| 50000|   Madrid| null|\n",
      "|       13|smoke salesman| 50000|Barcelona| null|\n",
      "|       14|     widgeteer| 35000|   Madrid| null|\n",
      "|       15|smoke salesman| 50000|Barcelona| null|\n",
      "|       16|    psychopath|100000|   Madrid| null|\n",
      "|       17|smoke salesman| 50000|Barcelona| null|\n",
      "|       18|    psychopath|100000|   Madrid| null|\n",
      "|       19|        wizard| 60000|Barcelona| null|\n",
      "+---------+--------------+------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with_locs.join(raises, on=['id_number', 'position'],how='left').show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+------+---------+---------+---------+------------------+\n",
      "|id_number|      position|salary| location| position|id_number|             raise|\n",
      "+---------+--------------+------+---------+---------+---------+------------------+\n",
      "|        0|     widgeteer| 35000|   Madrid|     null|     null|              null|\n",
      "|        1|     widgeteer| 35000|Barcelona|Barcelona|        1| 5449.414806032167|\n",
      "|        2|        wizard| 60000|Barcelona|     null|     null|              null|\n",
      "|        3|smoke salesman| 50000|   Madrid|     null|     null|              null|\n",
      "|        4|smoke salesman| 50000|   Madrid|   Madrid|        4|297.97219438070346|\n",
      "|        5|smoke salesman| 50000|Barcelona|     null|     null|              null|\n",
      "|        6|     widgeteer| 35000|   Madrid|     null|     null|              null|\n",
      "|        7|     widgeteer| 35000|Barcelona|     null|     null|              null|\n",
      "|        8|    psychopath|100000|   Madrid|     null|     null|              null|\n",
      "|        9|     widgeteer| 35000|Barcelona|     null|     null|              null|\n",
      "|       10|     widgeteer| 35000|   Madrid|     null|     null|              null|\n",
      "|       11|     widgeteer| 35000|Barcelona|     null|     null|              null|\n",
      "|       12|smoke salesman| 50000|   Madrid|     null|     null|              null|\n",
      "|       13|smoke salesman| 50000|Barcelona|Barcelona|       13|6498.8443777952325|\n",
      "|       14|     widgeteer| 35000|   Madrid|     null|     null|              null|\n",
      "|       15|smoke salesman| 50000|Barcelona|     null|     null|              null|\n",
      "|       16|    psychopath|100000|   Madrid|     null|     null|              null|\n",
      "|       17|smoke salesman| 50000|Barcelona|Barcelona|       17| 5053.552881033624|\n",
      "|       18|    psychopath|100000|   Madrid|     null|     null|              null|\n",
      "|       19|        wizard| 60000|Barcelona|     null|     null|              null|\n",
      "+---------+--------------+------+---------+---------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined = with_locs.join(raises, \n",
    "                        (with_locs['id_number'] == raises['id_number']) &\n",
    "                        (with_locs['location'] == raises['position']), \n",
    "                         how='left').cache()\n",
    "\n",
    "joined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Digression\n",
    "\n",
    "We can monitor our running jobs and storage used at the Spark Web UI. We can get its url with sc.uiWebUrl.\n",
    "\n",
    "StorageLevels represent how our DataFrame is cached: we can save the results of the computation up to that point, so that if we process several times the same data only the subsequent steps will be recomputed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can erase it with `unpersist`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id_number: bigint, position: string, salary: int, location: string, position: string, id_number: bigint, raise: double]"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Calculate the [z-score](http://www.statisticshowto.com/probability-and-statistics/z-score/) of each employee's salary for their location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Calculate the mean and std of salary for each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------------------+\n",
      "| location|avg_salary|        std_salary|\n",
      "+---------+----------+------------------+\n",
      "|   Madrid|   59000.0| 29040.20201949934|\n",
      "|Barcelona|   46000.0|10219.806477837263|\n",
      "+---------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "starting_point = with_locs.join(raises, on='id_number',how='left')\n",
    "\n",
    "stats = starting_point.groupby('location')\\\n",
    "              .agg(functions.mean('salary').alias('avg_salary'),\n",
    "                   functions.stddev('salary').alias('std_salary'))\n",
    "\n",
    "stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Annotate each employee with the stats corresponding to their location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- location: string (nullable = true)\n",
      " |-- id_number: long (nullable = false)\n",
      " |-- position: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- position: string (nullable = true)\n",
      " |-- raise: double (nullable = true)\n",
      " |-- avg_salary: double (nullable = true)\n",
      " |-- std_salary: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "annotated = starting_point.join(stats, on='location', how='left')\n",
    "annotated.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Calculate the z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|id_number|              zscore|\n",
      "+---------+--------------------+\n",
      "|        0| -0.8264405317802183|\n",
      "|        1| -1.0763413205381795|\n",
      "|        2|  1.3698889534122285|\n",
      "|        3|-0.30991519941758183|\n",
      "|        4|-0.30991519941758183|\n",
      "|        5| 0.39139684383206524|\n",
      "|        6| -0.8264405317802183|\n",
      "|        7| -1.0763413205381795|\n",
      "|        8|  1.4118359084578729|\n",
      "|        9| -1.0763413205381795|\n",
      "|       10| -0.8264405317802183|\n",
      "|       11| -1.0763413205381795|\n",
      "|       12|-0.30991519941758183|\n",
      "|       13| 0.39139684383206524|\n",
      "|       14| -0.8264405317802183|\n",
      "|       15| 0.39139684383206524|\n",
      "|       16|  1.4118359084578729|\n",
      "|       17| 0.39139684383206524|\n",
      "|       18|  1.4118359084578729|\n",
      "|       19|  1.3698889534122285|\n",
      "+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = annotated.withColumn('zscore', \n",
    "                                (annotated['salary'] - annotated['avg_salary']) / annotated['std_salary'])\n",
    "result.select('id_number', 'zscore').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id_number: bigint, position: string, salary: int, location: string]"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------+---------+\n",
      "|id_number|      position|  salary| location|\n",
      "+---------+--------------+--------+---------+\n",
      "|        0|     widgeteer|   35000|   Madrid|\n",
      "|        1|     widgeteer|   35000|Barcelona|\n",
      "|        2|        wizard|   60000|Barcelona|\n",
      "|        3|smoke salesman|   50000|   Madrid|\n",
      "|        4|smoke salesman|   50000|   Madrid|\n",
      "|        5|smoke salesman|   50000|Barcelona|\n",
      "|        6|     widgeteer|   35000|   Madrid|\n",
      "|        7|     widgeteer|   35000|Barcelona|\n",
      "|        8|    psychopath|  100000|   Madrid|\n",
      "|        9|     widgeteer|   35000|Barcelona|\n",
      "|       10|     widgeteer|   35000|   Madrid|\n",
      "|       11|     widgeteer|   35000|Barcelona|\n",
      "|       12|smoke salesman|   50000|   Madrid|\n",
      "|       13|smoke salesman|   50000|Barcelona|\n",
      "|       14|     widgeteer|   35000|   Madrid|\n",
      "|       15|smoke salesman|   50000|Barcelona|\n",
      "|       16|    psychopath|  100000|   Madrid|\n",
      "|       17|smoke salesman|   50000|Barcelona|\n",
      "|       18|    psychopath|  100000|   Madrid|\n",
      "|       19|        wizard|   60000|Barcelona|\n",
      "|      100|     superboss|12000000|     null|\n",
      "|      101|          null|  100000|    Miami|\n",
      "|      102|          null|    null|     null|\n",
      "|      103|          null|    null| Palencia|\n",
      "+---------+--------------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "the_others = session.createDataFrame([(100, 'superboss', 12000000, None),\n",
    "                                      (101, None, 100000, 'Miami'),\n",
    "                                      (102, None, None, None),\n",
    "                                      (103, None, None, 'Palencia'),\n",
    "                                     ],\n",
    "                                     )\n",
    "\n",
    "df = with_locs.union(the_others)\n",
    "df.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+------+---------+\n",
      "|id_number|      position|salary| location|\n",
      "+---------+--------------+------+---------+\n",
      "|        0|     widgeteer| 35000|   Madrid|\n",
      "|        1|     widgeteer| 35000|Barcelona|\n",
      "|        2|        wizard| 60000|Barcelona|\n",
      "|        3|smoke salesman| 50000|   Madrid|\n",
      "|        4|smoke salesman| 50000|   Madrid|\n",
      "|        5|smoke salesman| 50000|Barcelona|\n",
      "|        6|     widgeteer| 35000|   Madrid|\n",
      "|        7|     widgeteer| 35000|Barcelona|\n",
      "|        8|    psychopath|100000|   Madrid|\n",
      "|        9|     widgeteer| 35000|Barcelona|\n",
      "|       10|     widgeteer| 35000|   Madrid|\n",
      "|       11|     widgeteer| 35000|Barcelona|\n",
      "|       12|smoke salesman| 50000|   Madrid|\n",
      "|       13|smoke salesman| 50000|Barcelona|\n",
      "|       14|     widgeteer| 35000|   Madrid|\n",
      "|       15|smoke salesman| 50000|Barcelona|\n",
      "|       16|    psychopath|100000|   Madrid|\n",
      "|       17|smoke salesman| 50000|Barcelona|\n",
      "|       18|    psychopath|100000|   Madrid|\n",
      "|       19|        wizard| 60000|Barcelona|\n",
      "+---------+--------------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropna().show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------+---------+\n",
      "|id_number|      position|  salary| location|\n",
      "+---------+--------------+--------+---------+\n",
      "|        0|     widgeteer|   35000|   Madrid|\n",
      "|        1|     widgeteer|   35000|Barcelona|\n",
      "|        2|        wizard|   60000|Barcelona|\n",
      "|        3|smoke salesman|   50000|   Madrid|\n",
      "|        4|smoke salesman|   50000|   Madrid|\n",
      "|        5|smoke salesman|   50000|Barcelona|\n",
      "|        6|     widgeteer|   35000|   Madrid|\n",
      "|        7|     widgeteer|   35000|Barcelona|\n",
      "|        8|    psychopath|  100000|   Madrid|\n",
      "|        9|     widgeteer|   35000|Barcelona|\n",
      "|       10|     widgeteer|   35000|   Madrid|\n",
      "|       11|     widgeteer|   35000|Barcelona|\n",
      "|       12|smoke salesman|   50000|   Madrid|\n",
      "|       13|smoke salesman|   50000|Barcelona|\n",
      "|       14|     widgeteer|   35000|   Madrid|\n",
      "|       15|smoke salesman|   50000|Barcelona|\n",
      "|       16|    psychopath|  100000|   Madrid|\n",
      "|       17|smoke salesman|   50000|Barcelona|\n",
      "|       18|    psychopath|  100000|   Madrid|\n",
      "|       19|        wizard|   60000|Barcelona|\n",
      "|      100|     superboss|12000000|     null|\n",
      "|      101|          null|  100000|    Miami|\n",
      "|      103|          null|    null| Palencia|\n",
      "+---------+--------------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropna(how='all', subset=['salary', 'location']).show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+------+---------+\n",
      "|id_number|      position|salary| location|\n",
      "+---------+--------------+------+---------+\n",
      "|        0|     widgeteer| 35000|   Madrid|\n",
      "|        1|     widgeteer| 35000|Barcelona|\n",
      "|        2|        wizard| 60000|Barcelona|\n",
      "|        3|smoke salesman| 50000|   Madrid|\n",
      "|        4|smoke salesman| 50000|   Madrid|\n",
      "|        5|smoke salesman| 50000|Barcelona|\n",
      "|        6|     widgeteer| 35000|   Madrid|\n",
      "|        7|     widgeteer| 35000|Barcelona|\n",
      "|        8|    psychopath|100000|   Madrid|\n",
      "|        9|     widgeteer| 35000|Barcelona|\n",
      "|       10|     widgeteer| 35000|   Madrid|\n",
      "|       11|     widgeteer| 35000|Barcelona|\n",
      "|       12|smoke salesman| 50000|   Madrid|\n",
      "|       13|smoke salesman| 50000|Barcelona|\n",
      "|       14|     widgeteer| 35000|   Madrid|\n",
      "|       15|smoke salesman| 50000|Barcelona|\n",
      "|       16|    psychopath|100000|   Madrid|\n",
      "|       17|smoke salesman| 50000|Barcelona|\n",
      "|       18|    psychopath|100000|   Madrid|\n",
      "|       19|        wizard| 60000|Barcelona|\n",
      "|      101|          null|100000|    Miami|\n",
      "+---------+--------------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropna(how='any', subset=['salary', 'location']).show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------+---------+\n",
      "|id_number|      position|  salary| location|\n",
      "+---------+--------------+--------+---------+\n",
      "|        0|     widgeteer|   35000|   Madrid|\n",
      "|        1|     widgeteer|   35000|Barcelona|\n",
      "|        2|        wizard|   60000|Barcelona|\n",
      "|        3|smoke salesman|   50000|   Madrid|\n",
      "|        4|smoke salesman|   50000|   Madrid|\n",
      "|        5|smoke salesman|   50000|Barcelona|\n",
      "|        6|     widgeteer|   35000|   Madrid|\n",
      "|        7|     widgeteer|   35000|Barcelona|\n",
      "|        8|    psychopath|  100000|   Madrid|\n",
      "|        9|     widgeteer|   35000|Barcelona|\n",
      "|       10|     widgeteer|   35000|   Madrid|\n",
      "|       11|     widgeteer|   35000|Barcelona|\n",
      "|       12|smoke salesman|   50000|   Madrid|\n",
      "|       13|smoke salesman|   50000|Barcelona|\n",
      "|       14|     widgeteer|   35000|   Madrid|\n",
      "|       15|smoke salesman|   50000|Barcelona|\n",
      "|       16|    psychopath|  100000|   Madrid|\n",
      "|       17|smoke salesman|   50000|Barcelona|\n",
      "|       18|    psychopath|  100000|   Madrid|\n",
      "|       19|        wizard|   60000|Barcelona|\n",
      "|      100|     superboss|12000000|     null|\n",
      "|      101|          null|  100000|    Miami|\n",
      "+---------+--------------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropna(subset=['salary', 'location', 'position'], thresh=2).show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------+---------+\n",
      "|id_number|      position|  salary| location|\n",
      "+---------+--------------+--------+---------+\n",
      "|        0|     widgeteer|   35000|   Madrid|\n",
      "|        1|     widgeteer|   35000|Barcelona|\n",
      "|        2|        wizard|   60000|Barcelona|\n",
      "|        3|smoke salesman|   50000|   Madrid|\n",
      "|        4|smoke salesman|   50000|   Madrid|\n",
      "|        5|smoke salesman|   50000|Barcelona|\n",
      "|        6|     widgeteer|   35000|   Madrid|\n",
      "|        7|     widgeteer|   35000|Barcelona|\n",
      "|        8|    psychopath|  100000|   Madrid|\n",
      "|        9|     widgeteer|   35000|Barcelona|\n",
      "|       10|     widgeteer|   35000|   Madrid|\n",
      "|       11|     widgeteer|   35000|Barcelona|\n",
      "|       12|smoke salesman|   50000|   Madrid|\n",
      "|       13|smoke salesman|   50000|Barcelona|\n",
      "|       14|     widgeteer|   35000|   Madrid|\n",
      "|       15|smoke salesman|   50000|Barcelona|\n",
      "|       16|    psychopath|  100000|   Madrid|\n",
      "|       17|smoke salesman|   50000|Barcelona|\n",
      "|       18|    psychopath|  100000|   Madrid|\n",
      "|       19|        wizard|   60000|Barcelona|\n",
      "|      100|     superboss|12000000|     holi|\n",
      "|      101|          holi|  100000|    Miami|\n",
      "|      102|          holi|    null|     holi|\n",
      "|      103|          holi|    null| Palencia|\n",
      "+---------+--------------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.fillna('holi').show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------+---------+\n",
      "|id_number|      position|  salary| location|\n",
      "+---------+--------------+--------+---------+\n",
      "|        0|     widgeteer|   35000|   Madrid|\n",
      "|        1|     widgeteer|   35000|Barcelona|\n",
      "|        2|        wizard|   60000|Barcelona|\n",
      "|        3|smoke salesman|   50000|   Madrid|\n",
      "|        4|smoke salesman|   50000|   Madrid|\n",
      "|        5|smoke salesman|   50000|Barcelona|\n",
      "|        6|     widgeteer|   35000|   Madrid|\n",
      "|        7|     widgeteer|   35000|Barcelona|\n",
      "|        8|    psychopath|  100000|   Madrid|\n",
      "|        9|     widgeteer|   35000|Barcelona|\n",
      "|       10|     widgeteer|   35000|   Madrid|\n",
      "|       11|     widgeteer|   35000|Barcelona|\n",
      "|       12|smoke salesman|   50000|   Madrid|\n",
      "|       13|smoke salesman|   50000|Barcelona|\n",
      "|       14|     widgeteer|   35000|   Madrid|\n",
      "|       15|smoke salesman|   50000|Barcelona|\n",
      "|       16|    psychopath|  100000|   Madrid|\n",
      "|       17|smoke salesman|   50000|Barcelona|\n",
      "|       18|    psychopath|  100000|   Madrid|\n",
      "|       19|        wizard|   60000|Barcelona|\n",
      "|      100|     superboss|12000000|   casita|\n",
      "|      101|          null|  100000|    Miami|\n",
      "|      102|          null|    null|   casita|\n",
      "|      103|          null|    null| Palencia|\n",
      "+---------+--------------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.fillna('casita', subset=['location']).show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL querying\n",
    "\n",
    "We need to register our DataFrame as a table in the SQL context in order to be able to query against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Table or view not found: df; line 1 pos 14'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/master-p3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o24.sql.\n: org.apache.spark.sql.AnalysisException: Table or view not found: df; line 1 pos 14\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:663)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:615)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:645)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:638)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:638)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:584)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:123)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:117)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:638)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-383-94a082e7c023>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'''SELECT * from df'''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m         \"\"\"\n\u001b[0;32m--> 708\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/master-p3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Table or view not found: df; line 1 pos 14'"
     ]
    }
   ],
   "source": [
    "session.sql('''SELECT * from df''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable('df_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once registered, we can perform queries as complex as we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+------+--------+\n",
      "|id_number|      position|salary|location|\n",
      "+---------+--------------+------+--------+\n",
      "|        3|smoke salesman| 50000|  Madrid|\n",
      "|        4|smoke salesman| 50000|  Madrid|\n",
      "|        8|    psychopath|100000|  Madrid|\n",
      "|       12|smoke salesman| 50000|  Madrid|\n",
      "|       16|    psychopath|100000|  Madrid|\n",
      "|       18|    psychopath|100000|  Madrid|\n",
      "+---------+--------------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session.sql('''SELECT * from df_table \n",
    "               WHERE location=\"Madrid\" AND salary>40000''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------------------+\n",
      "|id_number|LOG1P(CAST(id_number AS DOUBLE))|\n",
      "+---------+--------------------------------+\n",
      "|        3|              1.3862943611198906|\n",
      "|        4|              1.6094379124341003|\n",
      "|        8|              2.1972245773362196|\n",
      "|       12|              2.5649493574615367|\n",
      "|       16|               2.833213344056216|\n",
      "|       18|              2.9444389791664403|\n",
      "+---------+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session.sql('''SELECT id_number, log1p(id_number) from df_table \n",
    "               WHERE location=\"Madrid\" AND salary>40000''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+----------------+\n",
      "|id_number|      position|classist(salary)|\n",
      "+---------+--------------+----------------+\n",
      "|        0|     widgeteer|     perroflauta|\n",
      "|        1|     widgeteer|     perroflauta|\n",
      "|        2|        wizard|         burgues|\n",
      "|        3|smoke salesman|         burgues|\n",
      "|        4|smoke salesman|         burgues|\n",
      "|        5|smoke salesman|         burgues|\n",
      "|        6|     widgeteer|     perroflauta|\n",
      "|        7|     widgeteer|     perroflauta|\n",
      "|        8|    psychopath|         burgues|\n",
      "|        9|     widgeteer|     perroflauta|\n",
      "|       10|     widgeteer|     perroflauta|\n",
      "|       11|     widgeteer|     perroflauta|\n",
      "|       12|smoke salesman|         burgues|\n",
      "|       13|smoke salesman|         burgues|\n",
      "|       14|     widgeteer|     perroflauta|\n",
      "|       15|smoke salesman|         burgues|\n",
      "|       16|    psychopath|         burgues|\n",
      "|       17|smoke salesman|         burgues|\n",
      "|       18|    psychopath|         burgues|\n",
      "|       19|        wizard|         burgues|\n",
      "+---------+--------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def classist(salary):\n",
    "    return 'perroflauta' if salary < 42000 else 'burgues'\n",
    "\n",
    "class_udf = functions.udf(classist)\n",
    "\n",
    "df.select('id_number', 'position', class_udf('salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"Undefined function: 'class_udf'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 1 pos 28\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/master-p3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o24.sql.\n: org.apache.spark.sql.AnalysisException: Undefined function: 'class_udf'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 1 pos 28\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$15$$anonfun$applyOrElse$49.apply(Analyzer.scala:1198)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$15$$anonfun$applyOrElse$49.apply(Analyzer.scala:1198)\n\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:48)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$15.applyOrElse(Analyzer.scala:1197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$15.applyOrElse(Analyzer.scala:1195)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsDown$1.apply(QueryPlan.scala:85)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsDown$1.apply(QueryPlan.scala:85)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:120)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:285)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:120)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:125)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDown(QueryPlan.scala:85)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressions(QueryPlan.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformAllExpressions$1.applyOrElse(QueryPlan.scala:136)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformAllExpressions$1.applyOrElse(QueryPlan.scala:135)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:256)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformAllExpressions(QueryPlan.scala:135)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:1195)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:1194)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)\n\tat scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:123)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:117)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:638)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-396-2e84877d10f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m session.sql('''SELECT id_number, position, class_udf(salary) \n\u001b[1;32m      2\u001b[0m                \u001b[0mFROM\u001b[0m \u001b[0mdf_table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                WHERE location=\"Madrid\" AND salary>40000''').show()\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m         \"\"\"\n\u001b[0;32m--> 708\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/master-p3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"Undefined function: 'class_udf'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 1 pos 28\""
     ]
    }
   ],
   "source": [
    "session.sql('''SELECT id_number, position, class_udf(salary) \n",
    "               FROM df_table \n",
    "               WHERE location=\"Madrid\" AND salary>40000''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------------------+\n",
      "|id_number|      position|classist_sql(salary)|\n",
      "+---------+--------------+--------------------+\n",
      "|        0|     widgeteer|         perroflauta|\n",
      "|        3|smoke salesman|             burgues|\n",
      "|        4|smoke salesman|             burgues|\n",
      "|        6|     widgeteer|         perroflauta|\n",
      "|        8|    psychopath|             burgues|\n",
      "|       10|     widgeteer|         perroflauta|\n",
      "|       12|smoke salesman|             burgues|\n",
      "|       14|     widgeteer|         perroflauta|\n",
      "|       16|    psychopath|             burgues|\n",
      "|       18|    psychopath|             burgues|\n",
      "+---------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session.udf.register('classist_sql', class_udf)\n",
    "\n",
    "session.sql('''SELECT id_number, position, classist_sql(salary)  \n",
    "               FROM df_table \n",
    "               WHERE location=\"Madrid\"''').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "\n",
    "replicate the previous exercise, but with SparkSQL instead of dataframe methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interoperation with Pandas\n",
    "\n",
    "Easy peasy. We can convert a spark DataFrame into a Pandas one, which will `collect` it, and viceversa, which will distribute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_number</th>\n",
       "      <th>position</th>\n",
       "      <th>salary</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>widgeteer</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>Madrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>widgeteer</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>Barcelona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>wizard</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>Barcelona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>smoke salesman</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>Madrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>smoke salesman</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>Madrid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_number        position   salary   location\n",
       "0          0       widgeteer  35000.0     Madrid\n",
       "1          1       widgeteer  35000.0  Barcelona\n",
       "2          2          wizard  60000.0  Barcelona\n",
       "3          3  smoke salesman  50000.0     Madrid\n",
       "4          4  smoke salesman  50000.0     Madrid"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df = df.toPandas()\n",
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id_number: bigint, position: string, salary: double, location: string]"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.csv('df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Repeat the exercise from the previous notebook, but this time with DataFrames.\n",
    "\n",
    "Get stats for all tickets with destination MAD from `coupons150720.csv`.\n",
    "\n",
    "You will need to extract ticket amounts with destination MAD, and then calculate:\n",
    "\n",
    "1. Total ticket amounts per origin\n",
    "2. Top 10 airlines by average amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------+---------+\n",
      "|id_number|      position|  salary| location|\n",
      "+---------+--------------+--------+---------+\n",
      "|      100|     superboss|12000000|     null|\n",
      "|       16|    psychopath|  100000|   Madrid|\n",
      "|       18|    psychopath|  100000|   Madrid|\n",
      "|        8|    psychopath|  100000|   Madrid|\n",
      "|      101|          null|  100000|    Miami|\n",
      "|       19|        wizard|   60000|Barcelona|\n",
      "|        2|        wizard|   60000|Barcelona|\n",
      "|       13|smoke salesman|   50000|Barcelona|\n",
      "|       12|smoke salesman|   50000|   Madrid|\n",
      "|        3|smoke salesman|   50000|   Madrid|\n",
      "|       17|smoke salesman|   50000|Barcelona|\n",
      "|        4|smoke salesman|   50000|   Madrid|\n",
      "|        5|smoke salesman|   50000|Barcelona|\n",
      "|       15|smoke salesman|   50000|Barcelona|\n",
      "|        9|     widgeteer|   35000|Barcelona|\n",
      "|        7|     widgeteer|   35000|Barcelona|\n",
      "|        0|     widgeteer|   35000|   Madrid|\n",
      "|       10|     widgeteer|   35000|   Madrid|\n",
      "|        1|     widgeteer|   35000|Barcelona|\n",
      "|        6|     widgeteer|   35000|   Madrid|\n",
      "+---------+--------------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort('salary', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Extract the fields you need (c0,c1,c2,c3,c4 and c6) into a dataframe with proper names and types\n",
    "\n",
    "Remember, you want to calculate:\n",
    "\n",
    "Total ticket amounts per origin\n",
    "\n",
    "Top 10 airlines by average amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79062005698500,1,MAA,AUH,9W,9W,56.79,USD,1,H,H,0526,150904,OK,IAF0\r\n",
      "79062005698500,2,AUH,CDG,9W,9W,84.34,USD,1,H,H,6120,150905,OK,IAF0\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 2 coupon150720.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[tkt_number: string, cpn_number: string, origin: string, destination: string, airline: string, amount: float]"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = session.sql('''SELECT \n",
    "                      _c0 AS tkt_number,\n",
    "                      _c1 AS cpn_number, \n",
    "                      _c2 AS origin, \n",
    "                      _c3 AS destination, \n",
    "                      _c4 AS airline, \n",
    "                      CAST(_c6 AS FLOAT) AS amount \n",
    "                    FROM csv.`coupon150720.csv`''')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(tkt_number='79062005639642', cpn_number='4', origin='BRU', destination='MAD', airline='UX', amount=21.020000457763672)"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.where(df['destination'] == 'MAD').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Total ticket amounts per origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|origin|       sum(amount)|\n",
      "+------+------------------+\n",
      "|   CCS| 94528.67986679077|\n",
      "|   GRU| 87192.63982868195|\n",
      "|   EZE| 81074.64043807983|\n",
      "|   BOG| 74644.44981968403|\n",
      "|   LHR| 69609.53006839752|\n",
      "|   LPA|60483.920072078705|\n",
      "|   MEX|56316.729888916016|\n",
      "|   JFK|53496.170244693756|\n",
      "|   TLV| 53436.21986293793|\n",
      "|   TFN| 50034.94993495941|\n",
      "|   LIM|48328.139808654785|\n",
      "|   MIA|45660.920145988464|\n",
      "|   SCL|  42584.9598236084|\n",
      "|   PMI| 40547.17005729675|\n",
      "|   FRA| 38863.54004433751|\n",
      "|   MUC| 37186.16008067131|\n",
      "|   DOH|34534.209548950195|\n",
      "|   BCN|32060.600192189217|\n",
      "|   FCO|30036.770029067993|\n",
      "|   AMS|29872.270063877106|\n",
      "+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gd = df.where(df['destination'] == 'MAD').groupby('origin')\n",
    "gd.sum('amount')\\\n",
    "  .sort('sum(amount)', ascending=False)\\\n",
    "  .show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Top 10 Airlines by average amount\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`mean(amount)`' given input columns: [airline, avg(amount)];;\\n'Sort ['mean(amount) DESC NULLS LAST], true\\n+- AnalysisBarrier\\n      +- Aggregate [airline#5871], [airline#5871, avg(cast(amount#5872 as double)) AS avg(amount)#6007]\\n         +- Filter (destination#5870 = MAD)\\n            +- Project [_c0#5883 AS tkt_number#5867, _c1#5884 AS cpn_number#5868, _c2#5885 AS origin#5869, _c3#5886 AS destination#5870, _c4#5887 AS airline#5871, cast(_c6#5889 as float) AS amount#5872]\\n               +- Relation[_c0#5883,_c1#5884,_c2#5885,_c3#5886,_c4#5887,_c5#5888,_c6#5889,_c7#5890,_c8#5891,_c9#5892,_c10#5893,_c11#5894,_c12#5895,_c13#5896,_c14#5897] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/master-p3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o3236.sort.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`mean(amount)`' given input columns: [airline, avg(amount)];;\n'Sort ['mean(amount) DESC NULLS LAST], true\n+- AnalysisBarrier\n      +- Aggregate [airline#5871], [airline#5871, avg(cast(amount#5872 as double)) AS avg(amount)#6007]\n         +- Filter (destination#5870 = MAD)\n            +- Project [_c0#5883 AS tkt_number#5867, _c1#5884 AS cpn_number#5868, _c2#5885 AS origin#5869, _c3#5886 AS destination#5870, _c4#5887 AS airline#5871, cast(_c6#5889 as float) AS amount#5872]\n               +- Relation[_c0#5883,_c1#5884,_c2#5885,_c3#5886,_c4#5887,_c5#5888,_c6#5889,_c7#5890,_c8#5891,_c9#5892,_c10#5893,_c11#5894,_c12#5895,_c13#5896,_c14#5897] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:120)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:120)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:125)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:80)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:80)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:104)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:172)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:178)\n\tat org.apache.spark.sql.Dataset$.apply(Dataset.scala:65)\n\tat org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3300)\n\tat org.apache.spark.sql.Dataset.sortInternal(Dataset.scala:3288)\n\tat org.apache.spark.sql.Dataset.sort(Dataset.scala:1177)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-432-f41ca51968d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'destination'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'MAD'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'airline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'amount'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mean(amount)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36msort\u001b[0;34m(self, *cols, **kwargs)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m         \"\"\"\n\u001b[0;32m--> 978\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sort_cols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/master-p3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`mean(amount)`' given input columns: [airline, avg(amount)];;\\n'Sort ['mean(amount) DESC NULLS LAST], true\\n+- AnalysisBarrier\\n      +- Aggregate [airline#5871], [airline#5871, avg(cast(amount#5872 as double)) AS avg(amount)#6007]\\n         +- Filter (destination#5870 = MAD)\\n            +- Project [_c0#5883 AS tkt_number#5867, _c1#5884 AS cpn_number#5868, _c2#5885 AS origin#5869, _c3#5886 AS destination#5870, _c4#5887 AS airline#5871, cast(_c6#5889 as float) AS amount#5872]\\n               +- Relation[_c0#5883,_c1#5884,_c2#5885,_c3#5886,_c4#5887,_c5#5888,_c6#5889,_c7#5890,_c8#5891,_c9#5892,_c10#5893,_c11#5894,_c12#5895,_c13#5896,_c14#5897] csv\\n\""
     ]
    }
   ],
   "source": [
    "gd = df.where(df['destination'] == 'MAD').groupby('airline')\n",
    "gd.mean('amount')\\\n",
    "  .sort('avg(amount)', ascending=False)\\\n",
    "  .show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/apache-spark-python\n",
    "\n",
    "https://spark.apache.org/docs/2.2.0/sql-programming-guide.html\n",
    "\n",
    "https://ogirardot.wordpress.com/2015/05/29/rdds-are-the-new-bytecode-of-apache-spark/\n",
    "\n",
    "https://stackoverflow.com/questions/36822224/what-are-the-pros-and-cons-of-parquet-format-compared-to-other-formats\n",
    "\n",
    "https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
